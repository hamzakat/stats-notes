[
  {
    "objectID": "stats-rethinking/07-chapter07.html",
    "href": "stats-rethinking/07-chapter07.html",
    "title": "7. Ulysses’ Compass",
    "section": "",
    "text": "7. Ulysses’ Compass",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "7. Ulysses' Compass"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html",
    "href": "stats-rethinking/05-chapter05.html",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "The dataset WaffleDivorce contains data om divorce rate, marraige rate, and median marriage age.\nLet’s load the data first\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(\"WaffleDivorce\")\nd &lt;- WaffleDivorce\n\nLet’s explore the associations:\n\nDivorce rate with median age at marriage\n\n\nplot(Divorce~MedianAgeMarriage, data=d, xlim=c(20,30), ylim=c(5,20))\n\n\n\n\n\n\n\n\n\nDivorce rate with marriage rate\n\n\nplot(Divorce~Marriage, data=d, xlim=c(10,30), ylim=c(5,20))\n\n\n\n\n\n\n\n\nIt looks like the associations are reversed.\n\n\nHere’s the model linear regression model for the 1st association: \\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_A A_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\nBefore coding the model, let’s standardize variables.\n\n# standardize variables\nd$D &lt;- standardize(d$Divorce)\nd$M &lt;- standardize(d$Marriage)\nd$A &lt;- standardize(d$MedianAgeMarriage)\n\nBy standardizing the variables, we can say that if \\(\\beta_A=1\\), then a change of 1 std. deviation in \\(A_i\\) is associated with a full std. deviation change in the outcome variable.\n\nm5.1 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a+bA*A,\n    a ~ dnorm(0, 0.2),\n    bA &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nset.seed(10)\nprior &lt;- extract.prior(m5.1)\nmu &lt;- link(m5.1, post=prior, data=list(A=c(-2,2)))\nplot(NULL, xlim=c(-2,2), ylim=c(-2,2))\nfor(i in 1:50) lines(c(-2,2), mu[i,], col=col.alpha(\"black\", 0.4))\n\n\n\n\n\n\n\n\n\nA_seq &lt;- seq(-3,3.2,30)\nmu &lt;- link(m5.1, data=list(A=A_seq))\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n{\n  plot(D~A, data=d, col=rangi2)\n  lines(A_seq, mu.mean, lwd=2)\n  shade(mu.PI, A_seq)\n}\n\n\n\n\n\n\n\n\n\n\n\n\\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_M M_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta_M \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\n\nm5.2 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a+bM*A,\n    a ~ dnorm(0, 0.2),\n    bM &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\n\n\nLet’s assume the possible causal models that are connected to the divorce rate association examples\n\n\n\nlibrary(dagitty)\nDMA1 &lt;- dagitty('dag{D &lt;- A -&gt; M -&gt; D}')\ndrawdag(DMA1)\n\n\n\n\n\n\n\n\n\nimpliedConditionalIndependencies(DMA1)\n# no output because there is no conditional independence\n\n\n\n\n\nDMA2 &lt;- dagitty('dag{D &lt;- A -&gt; M}')\ndrawdag(DMA2)\n\n\n\n\n\n\n\n\n\nimpliedConditionalIndependencies(DMA2)\n\nD _||_ M | A\n\n\n\n\n\n\n\\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_M M_i + \\beta_A A_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta_M \\sim N(0, 0.5)\n\\] \\[\n\\beta_A \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\n\nm5.3 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bM*M + bA*A ,\n    a ~ dnorm(0, 0.2),\n    bA &lt;- dnorm(0, 0.5),\n    bM &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\nprecis(m5.3)\n\n               mean         sd       5.5%      94.5%\na     -4.383618e-05 0.09707212 -0.1551838  0.1550962\nbA    -6.134983e-01 0.15097697 -0.8547887 -0.3722079\nbM    -6.546454e-02 0.15076527 -0.3064166  0.1754875\nsigma  7.850766e-01 0.07783322  0.6606841  0.9094691\n\n\n\nplot(coeftab(m5.1,m5.2,m5.3), par=c(\"bA\", \"bM\"))\n\n\n\n\n\n\n\n\nWe can read this result as: &gt; Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.\nIntuitive explanation: The plot shows that the coefficient for the median age at marriage (bA) is quite stable across models (m5.1 and m5.3), while the coefficient for the marriage rate (bM) is less stable and moves towards zero when both predictors are included. This indicates that the median age at marriage is capturing most of the variability that the marriage rate would also explain. Thus, once you know the median age at marriage, the marriage rate doesn’t add much new information (predictive power) for the outcome of interest.\nResults: - DAG 1 implies this result. - The association between marriage rate \\(M\\) and \\(D\\) divorce rate is spurious and caused by the influence of age of marriage on both \\(M\\) and \\(D\\). - Strictly speaking: \\(D \\amalg M|A\\)\n\n\n\nLet’s simulate the association in the DAG:\n\nN &lt;- 100\nx_real &lt;- rnorm(N)\nx_spur &lt;- rnorm(N, x_real)\ny &lt;- rnorm(N, x_real)\nd &lt;- data.frame(y, x_real, x_spur)\npairs(d)\n\n\n\n\n\n\n\n\n\n\n\nThe book covered 3 kinds of plots:\n\n\nPredictor residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest\n\ndata(\"WaffleDivorce\")\nd &lt;- WaffleDivorce\n\nd$D &lt;- standardize(d$Divorce)\nd$M &lt;- standardize(d$Marriage)\nd$A &lt;- standardize(d$MedianAgeMarriage)\n\nm5.4 &lt;- quap(\n  alist(\n    M ~ dnorm(mu, sigma),\n    mu &lt;- a + bAM * A, \n    a ~ dnorm(0, 0.2),\n    bAM ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ), data=d\n)\n\nmu &lt;- link(m5.4)\nmu_mean &lt;- apply(mu, 2, mean)\nmu_resid &lt;- d$M - mu_mean\n\n\n\n\n\nmu &lt;- link(m5.3)\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\n\nD_sim &lt;- sim(m5.3, n=1e4)\nD_PI &lt;- apply(D_sim, 2, PI)\n\n{\n  plot(mu_mean ~ d$D, col=rangi2, ylim=range(mu_PI),\n       xlab=\"Observed divorce\", ylab=\"Predicted divorce\")\n  \n  abline(a=0, b=1, lty=2)\n  for(i in 1:nrow(d)) lines(rep(d$D[i], 2), mu_PI[,i], col=rangi2)\n\n}\n\n\n\n\n\n\n\n\n\n\n\nThis displays the causal implications of the model. They help you understand the model, as well as generate predictions for imaginary interventions and compute how much some observed outcome could be attributed to some cause.\nThe basic recipe: 1. Set the assumed scientific model (e.g. draw the DAG) 2. Pick the intervention variable (i.e. variable to manipulate) 3. Define the range of values to set the intervention variable to 4. Do the following simulation:\nFor each value of intervention variable:\n  For each sample in posterior:\n    Use the causal model to simulate the values of other variables including the outcome\n\n\nFor the divorce model (represented by the DAG 1), we need a set of functions that tell us how each variable is generated. We will follow the same approach we did in the m5.3 but with adding the influence of A on M, since in the previous models we cared about estimating A -&gt; D influence. Now, we need to predict the consequences of manipulating A. Estimating the influence of A on M is conducting by regressing A on M.\n\nd &lt;- list()\nd$A &lt;- standardize(WaffleDivorce$MedianAgeMarriage)\nd$D &lt;- standardize(WaffleDivorce$Divorce)\nd$M &lt;- standardize(WaffleDivorce$Marriage)\n\nm5.3_A &lt;- quap(\n  alist(\n    # A -&gt; D &lt;- M\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bM * M + bA * A,\n    bM ~ dnorm(0, 0.5),\n    bA ~ dnorm(0, 0.5),\n    a ~ dnorm(0, 0.2),\n    sigma ~ dexp(1),\n    \n    # A -&gt; M\n    M ~ dnorm(mu_M, sigma_M),\n    mu_M &lt;- aM + bAM * A,\n    bAM ~ dnorm(0, 0.5),\n    aM ~ dnorm(0, 0.2),\n    sigma_M ~ dexp(1)\n    \n  ), data=d\n)\n\nLet’s define the range of values for A\n\nA_seq &lt;- seq(from=-2,to=2,length.out=30)\n\nLet’s do the simulate both M and D in order. The order is important because we have to simulate the influence of A -&gt; M before simulating the joint Influence A -&gt; M -&gt; D\n\nsim_dat &lt;- data.frame(A=A_seq)\n\ns &lt;- sim(m5.3_A, data=sim_dat, vars=c(\"M\",\"D\"))\n\n\n{\nplot(sim_dat$A, colMeans(s$D), ylim=c(-2,2), type=\"l\",\n     xlab=\"manipulated A\", ylab=\"counterfactual D\")\nshade(apply(s$D,2,PI), sim_dat$A)\nmtext(\"Total counterfactual effect of A on D\")\n}\n\n\n\n\n\n\n\n\nThis plot shows the predicted trend in D including both paths: A -&gt; D A -&gt; M -&gt; D\nLet’s find the expected causal effect of increasing median age at marriage from 20 to 30:\n\n# standardize 20 and 30 before inference (remember mean(A) = 26.1 and sd(A)=1.24)\nA_test &lt;- c(20,30) - 26.1 / 1.24\nsim2_dat &lt;- data.frame(A=A_test)\ns2 &lt;- sim(m5.3_A, data=sim2_dat, vars=c(\"M\",\"D\"))\n\n# find the expected causal effect on D: before increase - after increase\nmean(s2$D[, 2] - s2$D[,1])\n\n[1] -5.622089\n\n\nThe result indicates a huge effect (5.7 std. dev).\n\n\n\nLet’s simulate a counterfactual for an average state with A=0 and the causal effect of manipulating M on D (note: manipulating M requires removing the arrows entering into M resulting in the DAG model 2):\n\nM_seq &lt;- seq(from=-2,to=2,length.out=30)\n\nsim_dat &lt;- data.frame(M=M_seq, A=0)\n\ns &lt;- sim(m5.3_A, data=sim_dat, vars=\"D\")\n\n\n{\nplot(sim_dat$M, colMeans(s), ylim=c(-2,2), type=\"l\",\n     xlab=\"manipulated M\", ylab=\"counterfactual D\")\nshade(apply(s,2,PI), sim_dat$M)\nmtext(\"Total counterfactual effect of M on D\")\n}\n\n\n\n\n\n\n\n\nIt clear how the trend is less strong because there is no evidence for a strong influence of M on D.\n\n\n\nLet’s simulate the counterfactual for manipulating A w/o using the sim function.\n\nA_seq &lt;- seq(from=-2, to=-2, length.out=30)\npost &lt;- extract.samples(m5.3_A)\n\n# effect on M (distribution of M after simulating the counterfactual)\nM_sim &lt;- with(post, sapply(\n  1:30,\n  function(i) rnorm(1e3, aM + bAM*A_seq[i], sigma_M)\n))\n\ndens(M_sim)\n\n\n\n\n\n\n\n\n\n# effect on D\nD_sim &lt;- with(post, sapply(\n  1:30,\n  function(i) rnorm(1e3, a + bA*A_seq[i] + bM*M_sim[i], sigma_M)\n))\n\ndens(D_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(\"milk\")\nd &lt;- milk\nstr(d)\n\n'data.frame':   29 obs. of  8 variables:\n $ clade         : Factor w/ 4 levels \"Ape\",\"New World Monkey\",..: 4 4 4 4 4 2 2 2 2 2 ...\n $ species       : Factor w/ 29 levels \"A palliata\",\"Alouatta seniculus\",..: 11 8 9 10 16 2 1 6 28 27 ...\n $ kcal.per.g    : num  0.49 0.51 0.46 0.48 0.6 0.47 0.56 0.89 0.91 0.92 ...\n $ perc.fat      : num  16.6 19.3 14.1 14.9 27.3 ...\n $ perc.protein  : num  15.4 16.9 16.9 13.2 19.5 ...\n $ perc.lactose  : num  68 63.8 69 71.9 53.2 ...\n $ mass          : num  1.95 2.09 2.51 1.62 2.19 5.25 5.37 2.51 0.71 0.68 ...\n $ neocortex.perc: num  55.2 NA NA NA NA ...\n\n\nStandardiaze the variables we want to use in the analysis\n\nd$K &lt;- standardize(d$kcal.per.g)\nd$N &lt;- standardize(d$neocortex.perc)\nd$M &lt;- standardize(d$mass)\n\n\n\n\\[\nK_i \\sim Normal(\\mu_i, sigma)\n\\] \\[\n\\mu_i = \\alpha + \\beta_N N_i\n\\] Define the model with vague priors\n\nm5.5_draft &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,1),\n    bN~dnorm(0,1),\n    sigma~dexp(1)\n    \n  ), data=d\n)\n\nError in quap(alist(K ~ dnorm(mu, sigma), mu &lt;- a + bN * N, a ~ dnorm(0, : initial value in 'vmmin' is not finite\nThe start values for the parameters were invalid. This could be caused by missing values (NA) in the data or by start values outside the parameter constraints. If there are no NA values in the data, try using explicit start values.\n\n\nWe want to conduct the analysis with complete cases only (i.e. cases shouldn’t have NA in the variables of interest)\n\n# keep the rows that corresponds to complete cases for the variables we are interested in\ndcc &lt;- d[complete.cases(d$K, d$N, d$M),]\n\n\nm5.5_draft &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,1),\n    bN~dnorm(0,1),\n    sigma~dexp(1)\n    \n  ), data=dcc\n)\n\nLet’s simulate 50 priors:\n\nprior &lt;- extract.prior(m5.5_draft)\nxseq &lt;- c(-2,2)\nmu &lt;- link(m5.5_draft, post=prior, data = list(N=xseq))\n\n{\n  plot(NULL, xlim=xseq, ylim=xseq)\n  for (i in 1:50) lines(xseq, mu[i,], col=col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\nLet’s tighten the priors so that they stick closer and produce more reliable relationships:\n\nm5.5 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,0.2),\n    bN~dnorm(0,0.5),\n    sigma~dexp(1)\n    \n  ), data=dcc\n)\n\nprior &lt;- extract.prior(m5.5)\nxseq &lt;- c(-2,2)\nmu &lt;- link(m5.5, post=prior, data = list(N=xseq))\n\n{\n  plot(NULL, xlim=xseq, ylim=xseq)\n  for (i in 1:50) lines(xseq, mu[i,], col=col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\n\nxseq &lt;- seq(from=min(dcc$N)-0.15, to=max(dcc$N)+0.15, length.out=30)\nmu &lt;- link(m5.5, data=list(N=xseq))\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\nplot(K~N, data=dcc)\nlines(xseq, mu_mean, lwd=2)\nshade(mu_PI, xseq)\n\n\n\n\n\n\n\n\n\nprecis(m5.5)\n\n            mean        sd       5.5%     94.5%\na     0.03990624 0.1544899 -0.2069984 0.2868109\nbN    0.13324473 0.2237436 -0.2243407 0.4908302\nsigma 0.99980248 0.1647006  0.7365791 1.2630259\n\n\nN is slightly positivley associated with K.\n\n\n\nRemember that mass in the original dataset is the logarethem of mass (log-mass)\n\nm5.6 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bM * M,\n    a~dnorm(0,0.2),\n    bM~dnorm(0,0.5),\n    sigma~dexp(1)\n  ),\n  data=dcc\n)\nprecis(m5.6)\n\n             mean        sd       5.5%      94.5%\na      0.05300796 0.1515525 -0.1892022 0.29521809\nbM    -0.31900745 0.2238007 -0.6766842 0.03866927\nsigma  0.94932747 0.1574741  0.6976535 1.20100142\n\n\nLog-mass M is negatively associated with K.\n\nxseq &lt;- seq(from=min(dcc$M)-0.15, to=max(dcc$M)+0.15, length.out=30)\nmu &lt;- link(m5.6, data=list(M=xseq))\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\nplot(K~M, data=dcc)\nlines(xseq, mu_mean, lwd=2)\nshade(mu_PI, xseq)\n\n\n\n\n\n\n\n\nModel 1 and 2 aren’t useful in our case and they show no significant association\n\n\n\n\\[\nK_i \\sim Normal(\\mu_i, sigma)\n\\]\n\\[\n\\mu_i = \\alpha + \\beta_N N_i + \\beta_M M_i\n\\]\n\nm5.7 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN*N + bM * M,\n    a~dnorm(0,0.2),\n    bN~dnorm(0,0.5),\n    bM~dnorm(0,0.5),\n    sigma~dexp(1)\n  ),\n  data=dcc\n)\nprecis(m5.7)\n\n             mean        sd        5.5%      94.5%\na      0.06959672 0.1437680 -0.16017228  0.2993657\nbN     0.42039090 0.2322482  0.04921335  0.7915685\nbM    -0.56045516 0.2432229 -0.94917229 -0.1717380\nsigma  0.84041552 0.1448683  0.60888805  1.0719430\n\n\n\nplot(precis(m5.7))\n\n\n\n\n\n\n\n\nLet’s compare with the previous models:\n\nplot(coeftab(m5.5, m5.6, m5.7), pars=c(\"bM\",\"bN\"))\n\n\n\n\n\n\n\n\nBy incorporating both predictor variables in the regression, the posterior association of both with the outcome has increased. Also, the posterior means for N and M have both moved away from zero\n\npairs(~K + M + N, dcc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\n\nUsing the sex as a predictor for height, we have this model definition:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_m m_i\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_m \\sim Normal(0, 10)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nWhere \\(m_i\\) is an indicator variable that takes the value 1 if the case is male, and zero otherwise.\nThis implies that the prior would have more uncertainty for male cases. See how thhe prior for male is wider:\n\nmu_female &lt;- rnorm(1e4, 178, 20)\nmu_male &lt;- rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)\nprecis(data.frame(mu_female, mu_male), hist=FALSE)\n\n              mean       sd     5.5%    94.5%\nmu_female 177.9726 20.09901 146.2121 210.2413\nmu_male   177.8210 22.34856 141.8660 213.5161\n\n\nThis property for prior is not accepted in complex regression models.\nAnother way to encode categorical vars is using index variable, which is scalable to non-binary categories:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha_{SEX[i]}\\)\n\\(\\alpha_j \\sim Normal(178, 20) \\space for \\space j=1..2\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nNow, look how the same prior is assigned to each category. However, we need to construct the index variable as follows:\n\n# 2 if male\n# 1 if female\nd$sex &lt;- ifelse(d$male==1,2,1)\n\nm5.8 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a[sex],\n    a[sex] ~ dnorm(178,20),\n    sigma ~ dunif(0,50)\n  ),\n  data=d\n)\n\n# depth=2 is needed to show any vector parameters\nprecis(m5.8, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  134.91043 1.6069392 132.34223 137.47863\na[2]  142.57764 1.6974784 139.86474 145.29054\nsigma  27.31006 0.8280497  25.98668  28.63344\n\n\nWe can find the expected difference between females and males as follows:\n\npost &lt;- extract.samples(m5.8)\npost$diff_fm &lt;- post$a[,1] - post$a[,2]\nprecis(post,depth = 2)\n\n              mean        sd      5.5%      94.5%\nsigma    27.286991 0.8253867  25.97253  28.619264\na[1]    134.932750 1.6115325 132.37565 137.531772\na[2]    142.558122 1.6988628 139.86985 145.285992\ndiff_fm  -7.625372 2.3367162 -11.34310  -3.897871\n                                                                                                                       histogram\nsigma                   &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\na[1]                    &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\na[2]    &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\ndiff_fm                                         &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\n\n\nThis calculation is called a contrast.\n\n\n\n\ndata(milk)\n\nd &lt;- milk\n\nlevels(d$clade)\n\n[1] \"Ape\"              \"New World Monkey\" \"Old World Monkey\" \"Strepsirrhine\"   \n\n\n\nd$clade_id &lt;- as.integer(d$clade)\n\n\nd$K &lt;- standardize(d$kcal.per.g)\nm5.9 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a[clade_id],\n    a[clade_id] ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nlabels &lt;- paste(\"a[\", 1:4, \"]:\",levels(d$clade), sep=\"\")\n\nplot(precis(m5.9, depth=2, pars = \"a\"), labels=labels, xlab=\"expected kcal (std)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.In the divorce example, suppose the DAG is: M → A → D. What are the implied conditional independencies of the graph? Are the data consistent with it?\n\nlibrary(rethinking)\nlibrary(daggity)\n\nError in library(daggity): there is no package called 'daggity'\n\ndag &lt;- dagitty('dag{M -&gt; A -&gt; D}')\n\nimpliedConditionalIndependencies(dag)\n\nD _||_ M | A\n\n\nThis is the same implied conditional independency that the data is consistent with as discussed in the chapter.\n\nAssuming that the DAG for the divorce example is indeed M → A → D, fit a new model and use it to estimate the counterfactual effect of halving a State’s marriage rate M. Use the counterfactual example from the chapter (starting on page 140) as a template\n\nFit the new model\n\ndata(\"WaffleDivorce\")\n\nd &lt;- list()\nd$A &lt;- standardize(WaffleDivorce$MedianAgeMarriage)\nd$D &lt;- standardize(WaffleDivorce$Divorce)\nd$M &lt;- standardize(WaffleDivorce$Marriage)\n\n# M -&gt; A -&gt; D\nmH2 &lt;- quap(\n  alist(\n    # A -&gt; D\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bA * A,\n    a ~ dnorm(0,0.2),\n    bA ~ dnorm(0, 0.5),\n    sigma ~ dexp(1),\n    \n    # M -&gt; A\n    A ~ dnorm(mu_A, sigma_A),\n    mu_A &lt;- a_A + bM * M,\n    a_A ~ dnorm(0,0.2),\n    bM ~ dnorm(0, 0.5),\n    sigma_A ~ dexp(1)\n    \n  ),\n  data = d\n)\n\nSimulate the counterfactual effect of having a State’s marriage rate M\n\nM_seq &lt;- standardize(WaffleDivorce$Marriage * 0.5)\nM_seq &lt;- M_seq[order(M_seq)]\n\nsim_dat &lt;- data.frame(M=M_seq)\n\ns &lt;- sim(mH2, data=sim_dat, vars=c(\"A\",\"D\"))\n\n\n{\n  plot(sim_dat$M, colMeans(s$D), ylim=c(-2,2), type=\"l\",\n       xlab=\"manipulated M\", ylab=\"counterfactual D\")\n  shade(apply(s$D,2,PI), sim_dat$M)\n  mtext(\"Total counterfactual effect of M on D\")\n}",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#model-1-d-a",
    "href": "stats-rethinking/05-chapter05.html#model-1-d-a",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "Here’s the model linear regression model for the 1st association: \\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_A A_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\nBefore coding the model, let’s standardize variables.\n\n# standardize variables\nd$D &lt;- standardize(d$Divorce)\nd$M &lt;- standardize(d$Marriage)\nd$A &lt;- standardize(d$MedianAgeMarriage)\n\nBy standardizing the variables, we can say that if \\(\\beta_A=1\\), then a change of 1 std. deviation in \\(A_i\\) is associated with a full std. deviation change in the outcome variable.\n\nm5.1 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a+bA*A,\n    a ~ dnorm(0, 0.2),\n    bA &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nset.seed(10)\nprior &lt;- extract.prior(m5.1)\nmu &lt;- link(m5.1, post=prior, data=list(A=c(-2,2)))\nplot(NULL, xlim=c(-2,2), ylim=c(-2,2))\nfor(i in 1:50) lines(c(-2,2), mu[i,], col=col.alpha(\"black\", 0.4))\n\n\n\n\n\n\n\n\n\nA_seq &lt;- seq(-3,3.2,30)\nmu &lt;- link(m5.1, data=list(A=A_seq))\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n{\n  plot(D~A, data=d, col=rangi2)\n  lines(A_seq, mu.mean, lwd=2)\n  shade(mu.PI, A_seq)\n}",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#model-2-d-m",
    "href": "stats-rethinking/05-chapter05.html#model-2-d-m",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "\\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_M M_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta_M \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\n\nm5.2 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a+bM*A,\n    a ~ dnorm(0, 0.2),\n    bM &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#dags",
    "href": "stats-rethinking/05-chapter05.html#dags",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "Let’s assume the possible causal models that are connected to the divorce rate association examples\n\n\n\nlibrary(dagitty)\nDMA1 &lt;- dagitty('dag{D &lt;- A -&gt; M -&gt; D}')\ndrawdag(DMA1)\n\n\n\n\n\n\n\n\n\nimpliedConditionalIndependencies(DMA1)\n# no output because there is no conditional independence\n\n\n\n\n\nDMA2 &lt;- dagitty('dag{D &lt;- A -&gt; M}')\ndrawdag(DMA2)\n\n\n\n\n\n\n\n\n\nimpliedConditionalIndependencies(DMA2)\n\nD _||_ M | A",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#model-3-multiple-regression",
    "href": "stats-rethinking/05-chapter05.html#model-3-multiple-regression",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "\\[\nD_i \\sim N(\\mu_i, \\sigma)\n\\] \\[\n\\mu_i = \\sigma + \\beta_M M_i + \\beta_A A_i\n\\]\n\\[\n\\alpha \\sim N(0, 0.2)\n\\]\n\\[\n\\beta_M \\sim N(0, 0.5)\n\\] \\[\n\\beta_A \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma \\sim Exp(0, 0.5)\n\\]\n\nm5.3 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bM*M + bA*A ,\n    a ~ dnorm(0, 0.2),\n    bA &lt;- dnorm(0, 0.5),\n    bM &lt;- dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\nprecis(m5.3)\n\n               mean         sd       5.5%      94.5%\na     -4.383618e-05 0.09707212 -0.1551838  0.1550962\nbA    -6.134983e-01 0.15097697 -0.8547887 -0.3722079\nbM    -6.546454e-02 0.15076527 -0.3064166  0.1754875\nsigma  7.850766e-01 0.07783322  0.6606841  0.9094691\n\n\n\nplot(coeftab(m5.1,m5.2,m5.3), par=c(\"bA\", \"bM\"))\n\n\n\n\n\n\n\n\nWe can read this result as: &gt; Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.\nIntuitive explanation: The plot shows that the coefficient for the median age at marriage (bA) is quite stable across models (m5.1 and m5.3), while the coefficient for the marriage rate (bM) is less stable and moves towards zero when both predictors are included. This indicates that the median age at marriage is capturing most of the variability that the marriage rate would also explain. Thus, once you know the median age at marriage, the marriage rate doesn’t add much new information (predictive power) for the outcome of interest.\nResults: - DAG 1 implies this result. - The association between marriage rate \\(M\\) and \\(D\\) divorce rate is spurious and caused by the influence of age of marriage on both \\(M\\) and \\(D\\). - Strictly speaking: \\(D \\amalg M|A\\)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#simulating-spurious-association",
    "href": "stats-rethinking/05-chapter05.html#simulating-spurious-association",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "Let’s simulate the association in the DAG:\n\nN &lt;- 100\nx_real &lt;- rnorm(N)\nx_spur &lt;- rnorm(N, x_real)\ny &lt;- rnorm(N, x_real)\nd &lt;- data.frame(y, x_real, x_spur)\npairs(d)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#plotting-multivariate-posterior",
    "href": "stats-rethinking/05-chapter05.html#plotting-multivariate-posterior",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "The book covered 3 kinds of plots:\n\n\nPredictor residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest\n\ndata(\"WaffleDivorce\")\nd &lt;- WaffleDivorce\n\nd$D &lt;- standardize(d$Divorce)\nd$M &lt;- standardize(d$Marriage)\nd$A &lt;- standardize(d$MedianAgeMarriage)\n\nm5.4 &lt;- quap(\n  alist(\n    M ~ dnorm(mu, sigma),\n    mu &lt;- a + bAM * A, \n    a ~ dnorm(0, 0.2),\n    bAM ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n  ), data=d\n)\n\nmu &lt;- link(m5.4)\nmu_mean &lt;- apply(mu, 2, mean)\nmu_resid &lt;- d$M - mu_mean\n\n\n\n\n\nmu &lt;- link(m5.3)\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\n\nD_sim &lt;- sim(m5.3, n=1e4)\nD_PI &lt;- apply(D_sim, 2, PI)\n\n{\n  plot(mu_mean ~ d$D, col=rangi2, ylim=range(mu_PI),\n       xlab=\"Observed divorce\", ylab=\"Predicted divorce\")\n  \n  abline(a=0, b=1, lty=2)\n  for(i in 1:nrow(d)) lines(rep(d$D[i], 2), mu_PI[,i], col=rangi2)\n\n}\n\n\n\n\n\n\n\n\n\n\n\nThis displays the causal implications of the model. They help you understand the model, as well as generate predictions for imaginary interventions and compute how much some observed outcome could be attributed to some cause.\nThe basic recipe: 1. Set the assumed scientific model (e.g. draw the DAG) 2. Pick the intervention variable (i.e. variable to manipulate) 3. Define the range of values to set the intervention variable to 4. Do the following simulation:\nFor each value of intervention variable:\n  For each sample in posterior:\n    Use the causal model to simulate the values of other variables including the outcome\n\n\nFor the divorce model (represented by the DAG 1), we need a set of functions that tell us how each variable is generated. We will follow the same approach we did in the m5.3 but with adding the influence of A on M, since in the previous models we cared about estimating A -&gt; D influence. Now, we need to predict the consequences of manipulating A. Estimating the influence of A on M is conducting by regressing A on M.\n\nd &lt;- list()\nd$A &lt;- standardize(WaffleDivorce$MedianAgeMarriage)\nd$D &lt;- standardize(WaffleDivorce$Divorce)\nd$M &lt;- standardize(WaffleDivorce$Marriage)\n\nm5.3_A &lt;- quap(\n  alist(\n    # A -&gt; D &lt;- M\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bM * M + bA * A,\n    bM ~ dnorm(0, 0.5),\n    bA ~ dnorm(0, 0.5),\n    a ~ dnorm(0, 0.2),\n    sigma ~ dexp(1),\n    \n    # A -&gt; M\n    M ~ dnorm(mu_M, sigma_M),\n    mu_M &lt;- aM + bAM * A,\n    bAM ~ dnorm(0, 0.5),\n    aM ~ dnorm(0, 0.2),\n    sigma_M ~ dexp(1)\n    \n  ), data=d\n)\n\nLet’s define the range of values for A\n\nA_seq &lt;- seq(from=-2,to=2,length.out=30)\n\nLet’s do the simulate both M and D in order. The order is important because we have to simulate the influence of A -&gt; M before simulating the joint Influence A -&gt; M -&gt; D\n\nsim_dat &lt;- data.frame(A=A_seq)\n\ns &lt;- sim(m5.3_A, data=sim_dat, vars=c(\"M\",\"D\"))\n\n\n{\nplot(sim_dat$A, colMeans(s$D), ylim=c(-2,2), type=\"l\",\n     xlab=\"manipulated A\", ylab=\"counterfactual D\")\nshade(apply(s$D,2,PI), sim_dat$A)\nmtext(\"Total counterfactual effect of A on D\")\n}\n\n\n\n\n\n\n\n\nThis plot shows the predicted trend in D including both paths: A -&gt; D A -&gt; M -&gt; D\nLet’s find the expected causal effect of increasing median age at marriage from 20 to 30:\n\n# standardize 20 and 30 before inference (remember mean(A) = 26.1 and sd(A)=1.24)\nA_test &lt;- c(20,30) - 26.1 / 1.24\nsim2_dat &lt;- data.frame(A=A_test)\ns2 &lt;- sim(m5.3_A, data=sim2_dat, vars=c(\"M\",\"D\"))\n\n# find the expected causal effect on D: before increase - after increase\nmean(s2$D[, 2] - s2$D[,1])\n\n[1] -5.622089\n\n\nThe result indicates a huge effect (5.7 std. dev).\n\n\n\nLet’s simulate a counterfactual for an average state with A=0 and the causal effect of manipulating M on D (note: manipulating M requires removing the arrows entering into M resulting in the DAG model 2):\n\nM_seq &lt;- seq(from=-2,to=2,length.out=30)\n\nsim_dat &lt;- data.frame(M=M_seq, A=0)\n\ns &lt;- sim(m5.3_A, data=sim_dat, vars=\"D\")\n\n\n{\nplot(sim_dat$M, colMeans(s), ylim=c(-2,2), type=\"l\",\n     xlab=\"manipulated M\", ylab=\"counterfactual D\")\nshade(apply(s,2,PI), sim_dat$M)\nmtext(\"Total counterfactual effect of M on D\")\n}\n\n\n\n\n\n\n\n\nIt clear how the trend is less strong because there is no evidence for a strong influence of M on D.\n\n\n\nLet’s simulate the counterfactual for manipulating A w/o using the sim function.\n\nA_seq &lt;- seq(from=-2, to=-2, length.out=30)\npost &lt;- extract.samples(m5.3_A)\n\n# effect on M (distribution of M after simulating the counterfactual)\nM_sim &lt;- with(post, sapply(\n  1:30,\n  function(i) rnorm(1e3, aM + bAM*A_seq[i], sigma_M)\n))\n\ndens(M_sim)\n\n\n\n\n\n\n\n\n\n# effect on D\nD_sim &lt;- with(post, sapply(\n  1:30,\n  function(i) rnorm(1e3, a + bA*A_seq[i] + bM*M_sim[i], sigma_M)\n))\n\ndens(D_sim)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#masked-relationships",
    "href": "stats-rethinking/05-chapter05.html#masked-relationships",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "data(\"milk\")\nd &lt;- milk\nstr(d)\n\n'data.frame':   29 obs. of  8 variables:\n $ clade         : Factor w/ 4 levels \"Ape\",\"New World Monkey\",..: 4 4 4 4 4 2 2 2 2 2 ...\n $ species       : Factor w/ 29 levels \"A palliata\",\"Alouatta seniculus\",..: 11 8 9 10 16 2 1 6 28 27 ...\n $ kcal.per.g    : num  0.49 0.51 0.46 0.48 0.6 0.47 0.56 0.89 0.91 0.92 ...\n $ perc.fat      : num  16.6 19.3 14.1 14.9 27.3 ...\n $ perc.protein  : num  15.4 16.9 16.9 13.2 19.5 ...\n $ perc.lactose  : num  68 63.8 69 71.9 53.2 ...\n $ mass          : num  1.95 2.09 2.51 1.62 2.19 5.25 5.37 2.51 0.71 0.68 ...\n $ neocortex.perc: num  55.2 NA NA NA NA ...\n\n\nStandardiaze the variables we want to use in the analysis\n\nd$K &lt;- standardize(d$kcal.per.g)\nd$N &lt;- standardize(d$neocortex.perc)\nd$M &lt;- standardize(d$mass)\n\n\n\n\\[\nK_i \\sim Normal(\\mu_i, sigma)\n\\] \\[\n\\mu_i = \\alpha + \\beta_N N_i\n\\] Define the model with vague priors\n\nm5.5_draft &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,1),\n    bN~dnorm(0,1),\n    sigma~dexp(1)\n    \n  ), data=d\n)\n\nError in quap(alist(K ~ dnorm(mu, sigma), mu &lt;- a + bN * N, a ~ dnorm(0, : initial value in 'vmmin' is not finite\nThe start values for the parameters were invalid. This could be caused by missing values (NA) in the data or by start values outside the parameter constraints. If there are no NA values in the data, try using explicit start values.\n\n\nWe want to conduct the analysis with complete cases only (i.e. cases shouldn’t have NA in the variables of interest)\n\n# keep the rows that corresponds to complete cases for the variables we are interested in\ndcc &lt;- d[complete.cases(d$K, d$N, d$M),]\n\n\nm5.5_draft &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,1),\n    bN~dnorm(0,1),\n    sigma~dexp(1)\n    \n  ), data=dcc\n)\n\nLet’s simulate 50 priors:\n\nprior &lt;- extract.prior(m5.5_draft)\nxseq &lt;- c(-2,2)\nmu &lt;- link(m5.5_draft, post=prior, data = list(N=xseq))\n\n{\n  plot(NULL, xlim=xseq, ylim=xseq)\n  for (i in 1:50) lines(xseq, mu[i,], col=col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\nLet’s tighten the priors so that they stick closer and produce more reliable relationships:\n\nm5.5 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN * N,\n    a~dnorm(0,0.2),\n    bN~dnorm(0,0.5),\n    sigma~dexp(1)\n    \n  ), data=dcc\n)\n\nprior &lt;- extract.prior(m5.5)\nxseq &lt;- c(-2,2)\nmu &lt;- link(m5.5, post=prior, data = list(N=xseq))\n\n{\n  plot(NULL, xlim=xseq, ylim=xseq)\n  for (i in 1:50) lines(xseq, mu[i,], col=col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\n\nxseq &lt;- seq(from=min(dcc$N)-0.15, to=max(dcc$N)+0.15, length.out=30)\nmu &lt;- link(m5.5, data=list(N=xseq))\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\nplot(K~N, data=dcc)\nlines(xseq, mu_mean, lwd=2)\nshade(mu_PI, xseq)\n\n\n\n\n\n\n\n\n\nprecis(m5.5)\n\n            mean        sd       5.5%     94.5%\na     0.03990624 0.1544899 -0.2069984 0.2868109\nbN    0.13324473 0.2237436 -0.2243407 0.4908302\nsigma 0.99980248 0.1647006  0.7365791 1.2630259\n\n\nN is slightly positivley associated with K.\n\n\n\nRemember that mass in the original dataset is the logarethem of mass (log-mass)\n\nm5.6 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bM * M,\n    a~dnorm(0,0.2),\n    bM~dnorm(0,0.5),\n    sigma~dexp(1)\n  ),\n  data=dcc\n)\nprecis(m5.6)\n\n             mean        sd       5.5%      94.5%\na      0.05300796 0.1515525 -0.1892022 0.29521809\nbM    -0.31900745 0.2238007 -0.6766842 0.03866927\nsigma  0.94932747 0.1574741  0.6976535 1.20100142\n\n\nLog-mass M is negatively associated with K.\n\nxseq &lt;- seq(from=min(dcc$M)-0.15, to=max(dcc$M)+0.15, length.out=30)\nmu &lt;- link(m5.6, data=list(M=xseq))\nmu_mean &lt;- apply(mu, 2, mean)\nmu_PI &lt;- apply(mu, 2, PI)\nplot(K~M, data=dcc)\nlines(xseq, mu_mean, lwd=2)\nshade(mu_PI, xseq)\n\n\n\n\n\n\n\n\nModel 1 and 2 aren’t useful in our case and they show no significant association\n\n\n\n\\[\nK_i \\sim Normal(\\mu_i, sigma)\n\\]\n\\[\n\\mu_i = \\alpha + \\beta_N N_i + \\beta_M M_i\n\\]\n\nm5.7 &lt;- quap(\n  alist(\n    K~dnorm(mu, sigma),\n    mu &lt;- a + bN*N + bM * M,\n    a~dnorm(0,0.2),\n    bN~dnorm(0,0.5),\n    bM~dnorm(0,0.5),\n    sigma~dexp(1)\n  ),\n  data=dcc\n)\nprecis(m5.7)\n\n             mean        sd        5.5%      94.5%\na      0.06959672 0.1437680 -0.16017228  0.2993657\nbN     0.42039090 0.2322482  0.04921335  0.7915685\nbM    -0.56045516 0.2432229 -0.94917229 -0.1717380\nsigma  0.84041552 0.1448683  0.60888805  1.0719430\n\n\n\nplot(precis(m5.7))\n\n\n\n\n\n\n\n\nLet’s compare with the previous models:\n\nplot(coeftab(m5.5, m5.6, m5.7), pars=c(\"bM\",\"bN\"))\n\n\n\n\n\n\n\n\nBy incorporating both predictor variables in the regression, the posterior association of both with the outcome has increased. Also, the posterior means for N and M have both moved away from zero\n\npairs(~K + M + N, dcc)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#categorical-variables",
    "href": "stats-rethinking/05-chapter05.html#categorical-variables",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "library(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\n\nUsing the sex as a predictor for height, we have this model definition:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_m m_i\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_m \\sim Normal(0, 10)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nWhere \\(m_i\\) is an indicator variable that takes the value 1 if the case is male, and zero otherwise.\nThis implies that the prior would have more uncertainty for male cases. See how thhe prior for male is wider:\n\nmu_female &lt;- rnorm(1e4, 178, 20)\nmu_male &lt;- rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)\nprecis(data.frame(mu_female, mu_male), hist=FALSE)\n\n              mean       sd     5.5%    94.5%\nmu_female 177.9726 20.09901 146.2121 210.2413\nmu_male   177.8210 22.34856 141.8660 213.5161\n\n\nThis property for prior is not accepted in complex regression models.\nAnother way to encode categorical vars is using index variable, which is scalable to non-binary categories:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha_{SEX[i]}\\)\n\\(\\alpha_j \\sim Normal(178, 20) \\space for \\space j=1..2\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nNow, look how the same prior is assigned to each category. However, we need to construct the index variable as follows:\n\n# 2 if male\n# 1 if female\nd$sex &lt;- ifelse(d$male==1,2,1)\n\nm5.8 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a[sex],\n    a[sex] ~ dnorm(178,20),\n    sigma ~ dunif(0,50)\n  ),\n  data=d\n)\n\n# depth=2 is needed to show any vector parameters\nprecis(m5.8, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  134.91043 1.6069392 132.34223 137.47863\na[2]  142.57764 1.6974784 139.86474 145.29054\nsigma  27.31006 0.8280497  25.98668  28.63344\n\n\nWe can find the expected difference between females and males as follows:\n\npost &lt;- extract.samples(m5.8)\npost$diff_fm &lt;- post$a[,1] - post$a[,2]\nprecis(post,depth = 2)\n\n              mean        sd      5.5%      94.5%\nsigma    27.286991 0.8253867  25.97253  28.619264\na[1]    134.932750 1.6115325 132.37565 137.531772\na[2]    142.558122 1.6988628 139.86985 145.285992\ndiff_fm  -7.625372 2.3367162 -11.34310  -3.897871\n                                                                                                                       histogram\nsigma                   &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\na[1]                    &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\na[2]    &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\ndiff_fm                                         &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;\n\n\nThis calculation is called a contrast.\n\n\n\n\ndata(milk)\n\nd &lt;- milk\n\nlevels(d$clade)\n\n[1] \"Ape\"              \"New World Monkey\" \"Old World Monkey\" \"Strepsirrhine\"   \n\n\n\nd$clade_id &lt;- as.integer(d$clade)\n\n\nd$K &lt;- standardize(d$kcal.per.g)\nm5.9 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a[clade_id],\n    a[clade_id] ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nlabels &lt;- paste(\"a[\", 1:4, \"]:\",levels(d$clade), sep=\"\")\n\nplot(precis(m5.9, depth=2, pars = \"a\"), labels=labels, xlab=\"expected kcal (std)\")",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/05-chapter05.html#exercises",
    "href": "stats-rethinking/05-chapter05.html#exercises",
    "title": "5. The Many Variablles and The Spurious Waffles",
    "section": "",
    "text": "1.In the divorce example, suppose the DAG is: M → A → D. What are the implied conditional independencies of the graph? Are the data consistent with it?\n\nlibrary(rethinking)\nlibrary(daggity)\n\nError in library(daggity): there is no package called 'daggity'\n\ndag &lt;- dagitty('dag{M -&gt; A -&gt; D}')\n\nimpliedConditionalIndependencies(dag)\n\nD _||_ M | A\n\n\nThis is the same implied conditional independency that the data is consistent with as discussed in the chapter.\n\nAssuming that the DAG for the divorce example is indeed M → A → D, fit a new model and use it to estimate the counterfactual effect of halving a State’s marriage rate M. Use the counterfactual example from the chapter (starting on page 140) as a template\n\nFit the new model\n\ndata(\"WaffleDivorce\")\n\nd &lt;- list()\nd$A &lt;- standardize(WaffleDivorce$MedianAgeMarriage)\nd$D &lt;- standardize(WaffleDivorce$Divorce)\nd$M &lt;- standardize(WaffleDivorce$Marriage)\n\n# M -&gt; A -&gt; D\nmH2 &lt;- quap(\n  alist(\n    # A -&gt; D\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bA * A,\n    a ~ dnorm(0,0.2),\n    bA ~ dnorm(0, 0.5),\n    sigma ~ dexp(1),\n    \n    # M -&gt; A\n    A ~ dnorm(mu_A, sigma_A),\n    mu_A &lt;- a_A + bM * M,\n    a_A ~ dnorm(0,0.2),\n    bM ~ dnorm(0, 0.5),\n    sigma_A ~ dexp(1)\n    \n  ),\n  data = d\n)\n\nSimulate the counterfactual effect of having a State’s marriage rate M\n\nM_seq &lt;- standardize(WaffleDivorce$Marriage * 0.5)\nM_seq &lt;- M_seq[order(M_seq)]\n\nsim_dat &lt;- data.frame(M=M_seq)\n\ns &lt;- sim(mH2, data=sim_dat, vars=c(\"A\",\"D\"))\n\n\n{\n  plot(sim_dat$M, colMeans(s$D), ylim=c(-2,2), type=\"l\",\n       xlab=\"manipulated M\", ylab=\"counterfactual D\")\n  shade(apply(s$D,2,PI), sim_dat$M)\n  mtext(\"Total counterfactual effect of M on D\")\n}",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "5. The Many Variablles and The Spurious Waffles"
    ]
  },
  {
    "objectID": "stats-rethinking/03-chapter03.html",
    "href": "stats-rethinking/03-chapter03.html",
    "title": "3. Sampling the Imaginary",
    "section": "",
    "text": "Instead of relying on calculus to compute the posterior, we can use the sampling method. This chapter is about:\n\nHow to draw a sample\nSummarizing the posterior using the sample\n\nBoundaries\nProbability mass\nPoint Estimate\n\nSimulation using the sample\n\n\n\nBelow is the code for computing the posterior for the globe tossing model using grid approximation from chapter 2\n\np_grid &lt;- seq(from=0, to=1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\nposterior &lt;- likelihood * prior\n\nposterior &lt;- posterior / sum(posterior)\n\nNow, to draw a 10,000 samples from the posterior, we do this\n\nsamples &lt;- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)\n\nThe resulting samples are shown in this plot:\n\nplot(samples)\n\n\n\n\n\n\n\n\nAnd the following is density estimate computed from these samples:\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndens(samples)\n\n\n\n\n\n\n\n\nComparing with the density of the posterior computed via grid approxinmation, we found that it is very similar though it isn’t identical:\n\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\n\n\n\nWe prepared the model in the previous section. Now, we cam use it to summarize and interpret the posterior distribution. This is done by asking the model questions about the following:\n\n\nWhat is the probability (i.e. posterior probability) that the proportion of water is less than 0.5?\n\nUsing grid approx:\n\n\nsum(posterior[p_grid &lt; 0.5])\n\n[1] 0.1718746\n\n\n\nUsing samples from posterior\n\n\nsum(samples &lt; 0.5)/length(samples)\n\n[1] 0.1757\n\n\nWe can see that the results are very close to each other.\nHow much does posterior probability lie between 0.5 and 0.75?\n\nsum(samples &gt; 0.5 & samples &lt; 0.75) / length(samples)\n\n[1] 0.6034\n\n\nThis means that about 60% of the posterior probability lies between 0.5 and 0.75.\n\n\n\n\n\nIt is usually called:\n\nConfidence Interval in Frequentist stats\nCredible Interval in Bayesian stats\n\nHowever, the author calls it Compatibility Interval because:\n\nIt indicates a range of parameter values compatible with the model and data.\nHe doesn’t use the “confidence” term because the model, data, and interval may not inspire confidence\n\nWhat is the boundaries of parameter values (i.e. possible proportions of water) that holds the lower 80% posterior probability?\n\nquantile(samples, 0.8)\n\n      80% \n0.7587588 \n\n\nThe output 0.76 represents the stop point of the interval. So, the interval or parameters [0, 0.76] holds 80% of the posterior probability, i.e. the 80th percentile lies in it.\nThe boundaries of the middle 80% posterior probability\n\nquantile(samples, c(0.1, 0.9))\n\n      10%       90% \n0.4444444 0.8129129 \n\n\n\n\n\n\nAssign equal probability mass to each tail.\nCommon in scientific literature\nGood for summarizing the shape of distribution as long as it is not too asymmetrical\n\n\np_grid &lt;- seq(from=0, to=1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\n# observing 3 waters in 3 tosses\nlikelihood &lt;- dbinom(3, size=3, prob=p_grid)\n\nposterior &lt;- likelihood * prior\n\nposterior &lt;- posterior / sum(posterior)\n\nsamples &lt;- sample(p_grid, size=1e4, replace=TRUE, prob=posterior)\n\ndens(samples)\n\n\n\n\n\n\n\nPI(samples, prob=0.5)\n\n      25%       75% \n0.7087087 0.9319319 \n\n\nThe last line compute the PI assigning 25% of the probability mass to each end of the interval.\n\n\n\nHPDI: the narrowest interval containing the specified probability mass. It can be computed using this function with (prob=0.5) as probability mass:\n\nHPDI(samples, prob=0.5)\n\n     |0.5      0.5| \n0.8418418 1.0000000 \n\n\nThis means that the interval between the parameter values (i.e. proportion of water in our case) 0.56 and 0.75 has the highest posterior probability\nNotes:\n\nMost of the time, PI and HPDI are very similar except for the skewed distributions. It doesn’t matter which type of interval to use in bell shape curves.\nIf choice of interval type makes a big difference, then we shouldn’t be using them to summarize the posterior. PLOT THE ENTIRE POSTERIOR INSTEAD!\n\n\n\n\n\nIn Bayesian stats, parameter estimate = the entire posterior distribution != single number = function:\nParameter value -&gt; Posterior distribution (function) -&gt; Plausibility value\nWhy? Because this way we avoid discarding information about uncertainty in the entire posterior distribution.\nHowever, what if we want to produce a single point estimate to describe the posterior? Here are some common choices for doing that. Note that using single parameter value for making inference/prediction leads to overconfidence, so make sure to use the posterior.\n\n\nFrom the grid approximation:\n\np_grid[which.max(posterior)]\n\n[1] 1\n\n\nFrom the sample (it’s called the mode or MAP):\n\nchainmode(samples, adj=0.01)\n\n[1] 0.9671498\n\n\n\n\n\nLoss function is helpful to decide a single point estimate, here is how we do so:\n\nFirst, we must pick a loss function suitable to the problem.\nThen, we find the value that minimize the loss to use it as a single point estimate, i.e. the optimal point estimate\n\nCommon loss functions:\n\nAbsolute loss \\(| decision - true \\space value |\\) -&gt; median of the posterior is the optimal point estimate\nQuadratic loss \\((decision - true \\space value)^2\\) -&gt; mean of the posterior is the optimal point estimate\n\nNote: when the posterior distribution is symmetrical and normal looking = then the median and mean converge to the same point (i.e. it doesn’t matter which loss or point estimate to pick)\nExample:\n\nMedian\n\n\nmedian(samples)\n\n[1] 0.8418418\n\n\n\nThe expected loss when we decide that the proportion of water p=0.5 = sum of the weighted average loss:\n\n\nsum(posterior * abs(0.5 - p_grid))\n\n[1] 0.3128752\n\n\n\nWe can find the loss for every possible decision/value/proportion of water in p_grid:\n\n\nloss &lt;- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid)))\n\nAfter that, we can find the parameter/decision that minimizes the loss:\n\np_grid[which.min(loss)]\n\n[1] 0.8408408\n\n\nAnd this is actually the posterior median\n\n\n\n\nUsually, it is better to communicate as much as you can about:\n\nPosterior distribution\nData\nand Model\n\nso that others can build upon your work (Scientists thinking vs. Statistician thinking!)\n\n\n\n\n\n\nLikelihood functions work in both directions:\n\nGiven data, find how plausible it is: for Binomial, we use dbinom\nGiven the distribution and its parameters, simulate data (by sampling): for Binomial, we use rbinom\n\nEither way, Bayesian models are always generative, generate data through simulation or parameters through estimation.\nLet’s see in practice:\nUsing the true proportion of water on Earth prob=0.7, let’s find probability of observing 0, 1, or 2 water in 2 tosses:\n\ndbinom(0:2, size=2, prob = 0.7)\n\n[1] 0.09 0.42 0.49\n\n\nLet’s generate n=10 simulations/dummy observations with the same distribution properties. Remember, running a single simulation means tossing the earth size=2 times with prob=0.7 of observing water:\n\nrbinom(n=10, size=2, prob=0.7)\n\n [1] 2 1 2 2 1 1 2 1 2 1\n\n\nLet’s generate 100,000 dummy data (i.e. water observation) to verify that each value (0, 1, 2) appears in proportion to its likelihood:\n\ndummy_water &lt;- rbinom(n=1e5, size=2, prob=0.7)\ntable(dummy_water)/1e5\n\ndummy_water\n      0       1       2 \n0.08793 0.42011 0.49196 \n\n\nVery close to the computed likelihood. The difference is called the simulation variance and it is changed every execution.\n\ndummy_water &lt;- rbinom(n=1e5, size=100, prob=0.7)\nsimplehist(dummy_water, xlab=\"dummy water count\")\n\n\n\n\n\n\n\n\n\n\n\n\n# simulate for a single value of p\nw &lt;- rbinom(1e4, size=9, prob=0.6)\n\n# propogate parameter uncertainty into predictions\nw &lt;- rbinom(1e4, size=9, prob=samples)\nsimplehist(w)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_grid &lt;- seq(0, 1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\nlikelihood &lt;- dbinom(6, size = 9, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(prior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, prob = posterior, size=1e4, replace=TRUE)\n\n3E1. How much posterior probability lies below p = 0.2?\n\nsum(samples &lt; 0.2)/length(samples)\n\n[1] 4e-04\n\n# or mean(samples &lt; 0.2)\n\n3E2. How much posterior probability lies above p = 0.8?\n\nsum(samples &gt; 0.8)/length(samples)\n\n[1] 0.1116\n\n\n3E3. How much posterior probability lies between p = 0.2 and p = 0.8?\n\nsum(samples &gt;= 0.2 & samples &lt;= 0.8)/length(samples)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of p?\n\nquantile(posterior, 0.2)\n\n         20% \n1.277287e-06 \n\n\n3E5. 20% of the posterior probability lies above which value of p?\n\nquantile(posterior, 0.8)\n\n         80% \n0.0002219529 \n\n\n3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?\n3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "3. Sampling the Imaginary"
    ]
  },
  {
    "objectID": "stats-rethinking/03-chapter03.html#sampling-from-a-grid-approximate-posterior",
    "href": "stats-rethinking/03-chapter03.html#sampling-from-a-grid-approximate-posterior",
    "title": "3. Sampling the Imaginary",
    "section": "",
    "text": "Below is the code for computing the posterior for the globe tossing model using grid approximation from chapter 2\n\np_grid &lt;- seq(from=0, to=1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\nposterior &lt;- likelihood * prior\n\nposterior &lt;- posterior / sum(posterior)\n\nNow, to draw a 10,000 samples from the posterior, we do this\n\nsamples &lt;- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)\n\nThe resulting samples are shown in this plot:\n\nplot(samples)\n\n\n\n\n\n\n\n\nAnd the following is density estimate computed from these samples:\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndens(samples)\n\n\n\n\n\n\n\n\nComparing with the density of the posterior computed via grid approxinmation, we found that it is very similar though it isn’t identical:\n\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "3. Sampling the Imaginary"
    ]
  },
  {
    "objectID": "stats-rethinking/03-chapter03.html#sampling-to-summarize",
    "href": "stats-rethinking/03-chapter03.html#sampling-to-summarize",
    "title": "3. Sampling the Imaginary",
    "section": "",
    "text": "We prepared the model in the previous section. Now, we cam use it to summarize and interpret the posterior distribution. This is done by asking the model questions about the following:\n\n\nWhat is the probability (i.e. posterior probability) that the proportion of water is less than 0.5?\n\nUsing grid approx:\n\n\nsum(posterior[p_grid &lt; 0.5])\n\n[1] 0.1718746\n\n\n\nUsing samples from posterior\n\n\nsum(samples &lt; 0.5)/length(samples)\n\n[1] 0.1757\n\n\nWe can see that the results are very close to each other.\nHow much does posterior probability lie between 0.5 and 0.75?\n\nsum(samples &gt; 0.5 & samples &lt; 0.75) / length(samples)\n\n[1] 0.6034\n\n\nThis means that about 60% of the posterior probability lies between 0.5 and 0.75.\n\n\n\n\n\nIt is usually called:\n\nConfidence Interval in Frequentist stats\nCredible Interval in Bayesian stats\n\nHowever, the author calls it Compatibility Interval because:\n\nIt indicates a range of parameter values compatible with the model and data.\nHe doesn’t use the “confidence” term because the model, data, and interval may not inspire confidence\n\nWhat is the boundaries of parameter values (i.e. possible proportions of water) that holds the lower 80% posterior probability?\n\nquantile(samples, 0.8)\n\n      80% \n0.7587588 \n\n\nThe output 0.76 represents the stop point of the interval. So, the interval or parameters [0, 0.76] holds 80% of the posterior probability, i.e. the 80th percentile lies in it.\nThe boundaries of the middle 80% posterior probability\n\nquantile(samples, c(0.1, 0.9))\n\n      10%       90% \n0.4444444 0.8129129 \n\n\n\n\n\n\nAssign equal probability mass to each tail.\nCommon in scientific literature\nGood for summarizing the shape of distribution as long as it is not too asymmetrical\n\n\np_grid &lt;- seq(from=0, to=1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\n# observing 3 waters in 3 tosses\nlikelihood &lt;- dbinom(3, size=3, prob=p_grid)\n\nposterior &lt;- likelihood * prior\n\nposterior &lt;- posterior / sum(posterior)\n\nsamples &lt;- sample(p_grid, size=1e4, replace=TRUE, prob=posterior)\n\ndens(samples)\n\n\n\n\n\n\n\nPI(samples, prob=0.5)\n\n      25%       75% \n0.7087087 0.9319319 \n\n\nThe last line compute the PI assigning 25% of the probability mass to each end of the interval.\n\n\n\nHPDI: the narrowest interval containing the specified probability mass. It can be computed using this function with (prob=0.5) as probability mass:\n\nHPDI(samples, prob=0.5)\n\n     |0.5      0.5| \n0.8418418 1.0000000 \n\n\nThis means that the interval between the parameter values (i.e. proportion of water in our case) 0.56 and 0.75 has the highest posterior probability\nNotes:\n\nMost of the time, PI and HPDI are very similar except for the skewed distributions. It doesn’t matter which type of interval to use in bell shape curves.\nIf choice of interval type makes a big difference, then we shouldn’t be using them to summarize the posterior. PLOT THE ENTIRE POSTERIOR INSTEAD!\n\n\n\n\n\nIn Bayesian stats, parameter estimate = the entire posterior distribution != single number = function:\nParameter value -&gt; Posterior distribution (function) -&gt; Plausibility value\nWhy? Because this way we avoid discarding information about uncertainty in the entire posterior distribution.\nHowever, what if we want to produce a single point estimate to describe the posterior? Here are some common choices for doing that. Note that using single parameter value for making inference/prediction leads to overconfidence, so make sure to use the posterior.\n\n\nFrom the grid approximation:\n\np_grid[which.max(posterior)]\n\n[1] 1\n\n\nFrom the sample (it’s called the mode or MAP):\n\nchainmode(samples, adj=0.01)\n\n[1] 0.9671498\n\n\n\n\n\nLoss function is helpful to decide a single point estimate, here is how we do so:\n\nFirst, we must pick a loss function suitable to the problem.\nThen, we find the value that minimize the loss to use it as a single point estimate, i.e. the optimal point estimate\n\nCommon loss functions:\n\nAbsolute loss \\(| decision - true \\space value |\\) -&gt; median of the posterior is the optimal point estimate\nQuadratic loss \\((decision - true \\space value)^2\\) -&gt; mean of the posterior is the optimal point estimate\n\nNote: when the posterior distribution is symmetrical and normal looking = then the median and mean converge to the same point (i.e. it doesn’t matter which loss or point estimate to pick)\nExample:\n\nMedian\n\n\nmedian(samples)\n\n[1] 0.8418418\n\n\n\nThe expected loss when we decide that the proportion of water p=0.5 = sum of the weighted average loss:\n\n\nsum(posterior * abs(0.5 - p_grid))\n\n[1] 0.3128752\n\n\n\nWe can find the loss for every possible decision/value/proportion of water in p_grid:\n\n\nloss &lt;- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid)))\n\nAfter that, we can find the parameter/decision that minimizes the loss:\n\np_grid[which.min(loss)]\n\n[1] 0.8408408\n\n\nAnd this is actually the posterior median\n\n\n\n\nUsually, it is better to communicate as much as you can about:\n\nPosterior distribution\nData\nand Model\n\nso that others can build upon your work (Scientists thinking vs. Statistician thinking!)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "3. Sampling the Imaginary"
    ]
  },
  {
    "objectID": "stats-rethinking/03-chapter03.html#sampling-to-simulate-prediction",
    "href": "stats-rethinking/03-chapter03.html#sampling-to-simulate-prediction",
    "title": "3. Sampling the Imaginary",
    "section": "",
    "text": "Likelihood functions work in both directions:\n\nGiven data, find how plausible it is: for Binomial, we use dbinom\nGiven the distribution and its parameters, simulate data (by sampling): for Binomial, we use rbinom\n\nEither way, Bayesian models are always generative, generate data through simulation or parameters through estimation.\nLet’s see in practice:\nUsing the true proportion of water on Earth prob=0.7, let’s find probability of observing 0, 1, or 2 water in 2 tosses:\n\ndbinom(0:2, size=2, prob = 0.7)\n\n[1] 0.09 0.42 0.49\n\n\nLet’s generate n=10 simulations/dummy observations with the same distribution properties. Remember, running a single simulation means tossing the earth size=2 times with prob=0.7 of observing water:\n\nrbinom(n=10, size=2, prob=0.7)\n\n [1] 2 1 2 2 1 1 2 1 2 1\n\n\nLet’s generate 100,000 dummy data (i.e. water observation) to verify that each value (0, 1, 2) appears in proportion to its likelihood:\n\ndummy_water &lt;- rbinom(n=1e5, size=2, prob=0.7)\ntable(dummy_water)/1e5\n\ndummy_water\n      0       1       2 \n0.08793 0.42011 0.49196 \n\n\nVery close to the computed likelihood. The difference is called the simulation variance and it is changed every execution.\n\ndummy_water &lt;- rbinom(n=1e5, size=100, prob=0.7)\nsimplehist(dummy_water, xlab=\"dummy water count\")\n\n\n\n\n\n\n\n\n\n\n\n\n# simulate for a single value of p\nw &lt;- rbinom(1e4, size=9, prob=0.6)\n\n# propogate parameter uncertainty into predictions\nw &lt;- rbinom(1e4, size=9, prob=samples)\nsimplehist(w)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "3. Sampling the Imaginary"
    ]
  },
  {
    "objectID": "stats-rethinking/03-chapter03.html#practice",
    "href": "stats-rethinking/03-chapter03.html#practice",
    "title": "3. Sampling the Imaginary",
    "section": "",
    "text": "p_grid &lt;- seq(0, 1, length.out=1000)\nprior &lt;- rep(1, 1000)\n\nlikelihood &lt;- dbinom(6, size = 9, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(prior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, prob = posterior, size=1e4, replace=TRUE)\n\n3E1. How much posterior probability lies below p = 0.2?\n\nsum(samples &lt; 0.2)/length(samples)\n\n[1] 4e-04\n\n# or mean(samples &lt; 0.2)\n\n3E2. How much posterior probability lies above p = 0.8?\n\nsum(samples &gt; 0.8)/length(samples)\n\n[1] 0.1116\n\n\n3E3. How much posterior probability lies between p = 0.2 and p = 0.8?\n\nsum(samples &gt;= 0.2 & samples &lt;= 0.8)/length(samples)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of p?\n\nquantile(posterior, 0.2)\n\n         20% \n1.277287e-06 \n\n\n3E5. 20% of the posterior probability lies above which value of p?\n\nquantile(posterior, 0.8)\n\n         80% \n0.0002219529 \n\n\n3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?\n3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "3. Sampling the Imaginary"
    ]
  },
  {
    "objectID": "stats-rethinking/01-chapter01.html",
    "href": "stats-rethinking/01-chapter01.html",
    "title": "1. The Golem of Prague",
    "section": "",
    "text": "1. The Golem of Prague\nTODO",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "1. The Golem of Prague"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Statistics Notes\nSome study notes related to statistics, inference, and causality.\n\n\nResources\n\nSchmelzer, C. H., Martin Arnold, Alexander Gerber, and Martin. (2024). Introduction to Econometrics with R . https://www.econometrics-with-r.org/index.html\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition). CRC Press.\n\nLectures: https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus\nExercises: https://github.com/rmcelreath/stat_rethinking_2024\nhttps://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "stats-rethinking/02-chapter02.html",
    "href": "stats-rethinking/02-chapter02.html",
    "title": "2. Small Worlds and Large Worlds",
    "section": "",
    "text": "The likelihood of the data: 6 W in 9 tosses, under the probability of 0.5\n\ndbinom(6, size=9, prob=0.5)\n\n[1] 0.1640625\n\n\n\n\n\nFor the globe tossing example, let’s make a grid of 20 points\n\n# change this for more precision. however, there will not be much change in inference after 100 points\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- rep(1, num_of_points)\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nLet’s display the posterior distribution:\n\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\nDifferent prior #1:\n\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- ifelse(p_grid &lt; 0.5, 0, 1)\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n# 6. plot\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\nDifferent prior #2:\n\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- exp(-5*abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n# 6. plot\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\n\n\n\n\na.k.a. Gaussian Approximation, because we assume that the posterior is Gaussian\n\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nglobe.qa &lt;- quap(\n  \n  alist(\n    W ~ dbinom(W+L, p),  # binomial likelihood\n    p ~ dunif(0, 1)     # uniform prior\n  ),\n  \n  data=list(W=6, L=3)\n  \n)\n\n# print a summary of quadratic approx.\nprecis(globe.qa)\n\n       mean        sd      5.5%     94.5%\np 0.6666667 0.1571338 0.4155366 0.9177968\n\n\nThe output refers to the properties of the posterior, assuming it is Gaussian. Let’s compare this approximation with the real posterior distribution (it has a Beta distribution analytically)\n\nW &lt;- 6\nL &lt;- 3\n\n# from the previous outputs\nglobe.qa.mean &lt;- 0.67\nglobe.qa.sd &lt;- 0.16\n\n# the real posterior distribution\ncurve(dbeta(x, W+1, L+1), from=0, to=1)\n\n# the quadratic/gaussian approximation\ncurve(dnorm(x, globe.qa.mean, globe.qa.sd), lty=2, add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nn_samples &lt;- 1000\np &lt;- rep(NA, n_samples)\np[1] &lt;- 0.5\n\nW &lt;- 6\nL &lt;- 3\n\nfor (i in 2:n_samples) {\n  p_new &lt;- rnorm(1, p[i-1], 0.1)\n  if (p_new &lt; 0) p_new &lt;- ans(p_new)\n  if (p_new &gt; 1) p_new &lt;- 2 - p_new\n  q0 &lt;- dbinom(W, W+L, p[i-1])\n  q1 &lt;- dbinom(W, W+L, p_new)\n  p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1])\n}\n\n# compare the MCMC approximation with the analytical posterior (beta) \ndens(p, xlim=c(0,1))\ncurve(dbeta(x, W+1, L+1), lty=2, add=TRUE)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "2. Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "stats-rethinking/02-chapter02.html#example-p.-33",
    "href": "stats-rethinking/02-chapter02.html#example-p.-33",
    "title": "2. Small Worlds and Large Worlds",
    "section": "",
    "text": "The likelihood of the data: 6 W in 9 tosses, under the probability of 0.5\n\ndbinom(6, size=9, prob=0.5)\n\n[1] 0.1640625",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "2. Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "stats-rethinking/02-chapter02.html#grid-approximation",
    "href": "stats-rethinking/02-chapter02.html#grid-approximation",
    "title": "2. Small Worlds and Large Worlds",
    "section": "",
    "text": "For the globe tossing example, let’s make a grid of 20 points\n\n# change this for more precision. however, there will not be much change in inference after 100 points\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- rep(1, num_of_points)\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nLet’s display the posterior distribution:\n\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\nDifferent prior #1:\n\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- ifelse(p_grid &lt; 0.5, 0, 1)\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n# 6. plot\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")\n\n\n\n\n\n\n\n\nDifferent prior #2:\n\nnum_of_points = 20\n\n# 1. define grid\np_grid &lt;- seq(from=0, to=1, length.out=num_of_points)\n\n# 2. define prior\nprior &lt;- exp(-5*abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in the grid\nlikelihood &lt;- dbinom(6, size=9, prob=p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# 5. standarize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n# 6. plot\nplot(p_grid, posterior, type=\"b\", xlab = \"prob. of water\", ylab=\"posterior prob.\")",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "2. Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "stats-rethinking/02-chapter02.html#quadratic-approximation",
    "href": "stats-rethinking/02-chapter02.html#quadratic-approximation",
    "title": "2. Small Worlds and Large Worlds",
    "section": "",
    "text": "a.k.a. Gaussian Approximation, because we assume that the posterior is Gaussian\n\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nglobe.qa &lt;- quap(\n  \n  alist(\n    W ~ dbinom(W+L, p),  # binomial likelihood\n    p ~ dunif(0, 1)     # uniform prior\n  ),\n  \n  data=list(W=6, L=3)\n  \n)\n\n# print a summary of quadratic approx.\nprecis(globe.qa)\n\n       mean        sd      5.5%     94.5%\np 0.6666667 0.1571338 0.4155366 0.9177968\n\n\nThe output refers to the properties of the posterior, assuming it is Gaussian. Let’s compare this approximation with the real posterior distribution (it has a Beta distribution analytically)\n\nW &lt;- 6\nL &lt;- 3\n\n# from the previous outputs\nglobe.qa.mean &lt;- 0.67\nglobe.qa.sd &lt;- 0.16\n\n# the real posterior distribution\ncurve(dbeta(x, W+1, L+1), from=0, to=1)\n\n# the quadratic/gaussian approximation\ncurve(dnorm(x, globe.qa.mean, globe.qa.sd), lty=2, add=TRUE)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "2. Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "stats-rethinking/02-chapter02.html#markov-chain-monte-carlo-mcmc",
    "href": "stats-rethinking/02-chapter02.html#markov-chain-monte-carlo-mcmc",
    "title": "2. Small Worlds and Large Worlds",
    "section": "",
    "text": "n_samples &lt;- 1000\np &lt;- rep(NA, n_samples)\np[1] &lt;- 0.5\n\nW &lt;- 6\nL &lt;- 3\n\nfor (i in 2:n_samples) {\n  p_new &lt;- rnorm(1, p[i-1], 0.1)\n  if (p_new &lt; 0) p_new &lt;- ans(p_new)\n  if (p_new &gt; 1) p_new &lt;- 2 - p_new\n  q0 &lt;- dbinom(W, W+L, p[i-1])\n  q1 &lt;- dbinom(W, W+L, p_new)\n  p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1])\n}\n\n# compare the MCMC approximation with the analytical posterior (beta) \ndens(p, xlim=c(0,1))\ncurve(dbeta(x, W+1, L+1), lty=2, add=TRUE)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "2. Small Worlds and Large Worlds"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html",
    "href": "stats-rethinking/04-chapter04.html",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "(Keep in mind the example of field soccer, coin tossing, and stepping right and left on page 72)\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n# given that the steps for each person is represented by a list of 16 random numbers\n# between -1 and 1:\n# run 1000 simulation of stepping left and right, and store the final result/position\npos &lt;- replicate(1000, sum(runif(16,-1,1)))\n\n# plot the end positions around the half line of soccer field\nplot(pos)\n\n\n\n\n\n\n\n# plot the density of the positions\nplot(density(pos))\n\n\n\n\n\n\n\n\n\n\n\n(See the example on page 74) Note that the interaction between the growth deviations converges to Gaussian dist as long as the effect is small.\n\ngrowth_samll_effect &lt;- replicate(1000, prod(1+runif(12,0,0.1)))\ndens(growth_samll_effect, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\ngrowth_big_effect &lt;- replicate(10000, prod(1+runif(12,0,0.5)))\ndens(growth_big_effect, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nMultiplication interactions of large deviations converges to Gaussian dist when we measure the outcomes on the log scale.\n\nlog.big &lt;- replicate(10000, log(prod(1+runif(12,0,0.5))))\ndens(log.big, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\n\n# explore data\nstr(d)\n\n'data.frame':   544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n\n\n\n# data summary\nprecis(d, hist=FALSE)\n\n              mean         sd      5.5%     94.5%\nheight 138.2635963 27.6024476 81.108550 165.73500\nweight  35.6106176 14.7191782  9.360721  54.50289\nage     29.3443934 20.7468882  1.000000  66.13500\nmale     0.4724265  0.4996986  0.000000   1.00000\n\n\n\n\n\nBased on domain-specific information, we decide that the range of plausible human heights is \\(178 \\mp 40\\). The std. deviation must be basically positive \\(h_i \\sim Normal(\\mu, \\sigma)\\)\nGiven that the parameters are independent, the prior is:\n\\(Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)\\)\nWhere:\n\\(\\mu \\sim Normal(178,20)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\nLet’s plot the priors: - Mean\n\ncurve(dnorm(x, 178, 20), from=100, to=250)\n\n\n\n\n\n\n\n\n\nStd. deviation\n\n\ncurve(dunif(x, 0, 50), from=0, to=60)\n\n\n\n\n\n\n\n\nLet’s simulate heights based on the priors. This is called the prior predictive simulation:\n\nsample_mu &lt;- rnorm(1e4, 178, 20)\nsample_sigma &lt;- runif(1e4, 0, 50)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\n\n\n\n\nSo far, the model is defined before showing it the data. We can change the prior \\(\\mu\\) std. deviation to see how the model is sensitive to the prior choices that aren’t relying on scintific knowledge as we did.\n\nsample_mu &lt;- rnorm(1e4, 178, 100)\nsample_sigma &lt;- runif(1e4, 0, 50)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\n\n\n\n\nNote how the result doesn’t make sense with negative and very large heights.\n\n\n\nSince we have 2 parameters, grid approx. method is not practical. However, we will try using it computing the log-likelihood:\n\n# we will compute the approximation using the height data of persons over 18 y.o.\nd2 &lt;- d[d$age &gt;= 18,]\n\n\nmu.list &lt;- seq(from=150, to=160, length.out=100)\nsigma.list &lt;- seq(from=7, to=9, length.out=100)\n\n# Create a Data Frame from All Combinations of Factor Variables\npost &lt;- expand.grid(mu=mu.list, sigma=sigma.list)\n\n# compute the log-likelihood\npost$LL &lt;- sapply(\n  1:nrow(post),\n  function(i) sum (\n    dnorm(d2$height, post$mu[i], post$sigma[i], log=TRUE)\n  )\n)\n\npost$prod &lt;- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)\n\n\npost$prob &lt;- exp(post$prod - max(post$prod))\ncontour_xyz(post$mu, post$sigma, post$prob)\n\n\n\n\n\n\n\n\n\nimage_xyz(post$mu, post$sigma, post$prob)\n\n\n\n\n\n\n\n\n\n\n\n\n# generate random indexes of rows\nsample.rows &lt;- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob)\nsample.mu &lt;- post$mu[sample.rows]\nsample.sigma &lt;- post$sigma[sample.rows]\n\n# this shows the most plausible combinations of mu and sigma\nplot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nLet’s check the shape of marginal posterior densities:\n\ndens(sample.mu)\n\n\n\n\n\n\n\ndens(sample.sigma)\n\n\n\n\n\n\n\n\nNote that the density for sigma has a longer right tail\n\nd3 &lt;- sample(d2$height, size = 20)\n\nmu.list &lt;- seq( from=150, to=170 , length.out=200 ) \nsigma.list &lt;- seq( from=4 , to=20 , length.out=200 ) \npost2 &lt;- expand.grid( mu=mu.list , sigma=sigma.list ) \npost2$LL &lt;- sapply( 1:nrow(post2) , function(i) sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) ) \npost2$prod &lt;- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) \npost2$prob &lt;- exp( post2$prod - max(post2$prod) ) \n\nsample2.rows &lt;- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) \nsample2.mu &lt;- post2$mu[ sample2.rows ] \nsample2.sigma &lt;- post2$sigma[ sample2.rows ] \nplot( sample2.mu , sample2.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab=\"mu\" , ylab=\"sigma\" , pch=16 )\n\n\n\n\n\n\n\n\n\ndens(sample2.sigma, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Approximation is good to make inferences about the shape of posterior, particularly its peak that lie at the maximum a posteriori (MAP)\nLet’s first load the data:\n\n# load data\nlibrary(rethinking) \ndata(Howell1) \nd &lt;- Howell1 \nd2 &lt;- d[ d$age &gt;= 18 , ]\n\nNow, we will define our model with code:\n\\(h_i \\sim Normal(\\mu, \\sigma)\\)\n\\(\\mu \\sim Normal(178,20)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\n\nflist &lt;- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\nNote: alist stores formulas without executing the expression in the code unlike list\nNow, we fit the mode to the data in `d2`:\n\nm4.1 &lt;- quap(flist, data=d2)\n\nLet’s take a glance at the model (i.e posterior dist):\n\nprecis(m4.1)\n\n            mean        sd       5.5%      94.5%\nmu    154.607026 0.4119947 153.948579 155.265473\nsigma   7.731333 0.2913861   7.265642   8.197025\n\n\n\n\n\n\npost &lt;- extract.samples(m4.1, n=1e4)\nhead(post)\n\n        mu    sigma\n1 154.8719 8.369672\n2 155.0301 8.009201\n3 154.9311 7.360071\n4 154.3732 7.744091\n5 154.7560 7.226322\n6 155.2402 7.903876\n\n\n\nprecis(post, hist=FALSE)\n\n           mean        sd       5.5%      94.5%\nmu    154.60282 0.4124339 153.945159 155.261959\nsigma   7.72796 0.2908551   7.255576   8.187313\n\n\nComparing these values to the output from precis(m4.1), we found it very close.\n\nplot(post)\n\n\n\n\n\n\n\n\n\n\nThe function extract.samples runs the following simulation that samples random vectors of multivariate Gaussian values. This simulation requires computing the variance-covariance matrix\n\nlibrary(MASS)\npost &lt;- mvrnorm(n=1e4, mu=coef(m4.1), Sigma=vcov(m4.1))\nplot(post)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is an essential compnent in the quap algorithm.\nIt tells us how each parameter relates to every other parameter in the posterior distribution.\nIt can be factored into 2 elements:\n\nVector of variances for the parameters diag(vcov(model))\nCorrelation matrix that tells how changes in one parameter lead to correlated changes in the others cov2cor(vcov(model))\n\n\n\n\n\n\nUsing the association between predictor variables and outcome variable, we want to predict the later. This is how linear regression works.\n\n\n\nWe tell the model (golem) the following: “Assume that the predictor variable has a constant and additive relationship to the mean of the outcome. Consider all the lines (formed by the combinations of parameter values) that relate one variable (or more) to the other. Rank all of these lines by plausibility, given these data.”\nThe resulted model is a posterior distribution\n\nIn the following example, we want to predict the height using the weight as a predictor variable. This code plot the data to use in model fitting:\n\nlibrary(rethinking)\ndata(Howell1)\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18,]\n\nplot(d2$height ~ d2$weight)\n\n\n\n\n\n\n\n\nWe want to use the Gaussian model of height we built in the previous chapters but making the mean of height \\(\\mu_i\\) is a function of weights where weight values are denoted by \\(x\\). Here is the model:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta(x_i - \\bar{x})\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta \\sim Normal(0, 10)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nNotations:\n\n\\(\\bar{x}\\) is the mean of weights\n\\(x_i\\) weight at row \\(i\\)\n\\(\\mu_i\\) the mean of heights are row \\(i\\)\n\\(h_i\\) the height at row \\(i\\)\n\\(\\alpha, \\beta\\) are parameters to learn\n\nNote all relationships are Stochastic except the relationship between the height mean and weight.\nThe parameters are made up as devices that will help us to manipulate \\(\\mu\\). Here is what each parameter does:\n\n\\(\\alpha\\) (intercept): represents the expected height when \\(x_i=\\bar{x}\\)\n\\(\\beta\\) (slope): represents the rate of change in expectation when \\(x_i\\) changes by 1 unit\n\n\n\n\n\nThe unobserved variables are called parameters (\\(\\alpha, \\beta, \\sigma\\)) and their distributions are called priors.\nEach combination of parameter values implies a unique line\nLet’s simulate the prior predictive distribution to see the possible lines\n\n\nset.seed(2971)\nN &lt;- 100 # 100 lines\na &lt;- rnorm(N, 178, 20)\nb &lt;- rnorm(N, 0, 10)\n\n# prepare the canvas for plotting\nplot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=\"weight\", ylab=\"height\")\nabline(h=0, lty=2) # no one is shorter than zero!\nabline(h=272, lty=1, lwd=0.5) # the world's tallest person\n\nxbar &lt;- mean(d2$weight)\n\n# simulate the possible lines\nfor (i in 1:N) curve(a[i] + b[i]*(x-xbar), \n                     from=min(d2$weight),\n                     to=max(d2$weight),\n                     add=TRUE,\n                     col=col.alpha(\"black\", 0.2))\n\n\n\n\n\n\n\n\nAs we can see, not all the lines seem to represent the relationship between weight and height for human. Negative relationship doesn’t make sense in this context.\nWe want to restrict \\(\\beta\\) to positive numbers so we only get positive relationship. Therefore, we can define the prior as Log-Normal instead to enforce positive relationship:\n\\[\n\\beta \\sim Log-Normal(0,1)\n\\]\n\nb &lt;- rlnorm(1e4, 0, 1)\ndens(b, xlim=c(-1,5), adj=0.1)\n\n\n\n\n\n\n\n\nWe can see the distribution is defined only on the positive beta values.\nNow, let’s do the prior predictive simulation again with the new prior:\n\nset.seed(2971)\nN &lt;- 100 # 100 lines\na &lt;- rnorm(N, 178, 20)\nb &lt;- rlnorm(N, 0, 1) # log-normal prior\n\n# prepare the canvas for plotting\nplot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=\"weight\", ylab=\"height\")\nabline(h=0, lty=2) # no one is shorter than zero!\nabline(h=272, lty=1, lwd=0.5) # the world's tallest person\n\nxbar &lt;- mean(d2$weight)\n\n# simulate the possible lines\nfor (i in 1:N) curve(a[i] + b[i]*(x-xbar), \n                     from=min(d2$weight),\n                     to=max(d2$weight),\n                     add=TRUE,\n                     col=col.alpha(\"black\", 0.2))\n\n\n\n\n\n\n\n\nNow, the result is much more sensible!\n\n\n\nThe model is defined now along with the priors. We are now ready to build the posterior approximation using quap\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18,]\n\nxbar &lt;- mean(d2$weight)\n\n# fit the model\n\nm4.3 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b * (weight-xbar),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data=d2\n)\n\nTo interpret the posterior, we can use either tables or plots. Plots gives more information about the posterior. However, let’s see the summary table:\n\nprecis(m4.3)\n\n             mean         sd        5.5%       94.5%\na     154.6013671 0.27030766 154.1693633 155.0333710\nb       0.9032807 0.04192363   0.8362787   0.9702828\nsigma   5.0718809 0.19115478   4.7663786   5.3773831\n\n\nWe also need to see the covariance among the parameters by computing the variance-covariance matrix:\n\nround(vcov(m4.3), 3)\n\n          a     b sigma\na     0.073 0.000 0.000\nb     0.000 0.002 0.000\nsigma 0.000 0.000 0.037\n\n\n\n\n\nplot(height~weight, data=d2, col=rangi2)\npost &lt;- extract.samples(m4.3)\na_map &lt;- mean(post$a)\nb_map &lt;- mean(post$b)\ncurve(a_map + b_map * (x-xbar), add=TRUE)\n\n\n\n\n\n\n\npost[1:5,]\n\n         a         b    sigma\n1 154.5564 0.9000245 4.768499\n2 154.4602 0.8474034 5.136055\n3 154.7858 0.9440438 5.313512\n4 154.4490 0.8806246 4.683555\n5 154.4639 0.8397176 5.185017\n\n\n\n\n\nWe want to know the uncertainty around the mean of posterior in order to determine the confidence in the relationship between predictor and outcome, since the posterior we plot in the previous step is the MAP, which is the mean of many lines formed by the posterior.\nHere is a sample of possible lines:\n\npost[1:5,]\n\n         a         b    sigma\n1 154.5564 0.9000245 4.768499\n2 154.4602 0.8474034 5.136055\n3 154.7858 0.9440438 5.313512\n4 154.4490 0.8806246 4.683555\n5 154.4639 0.8397176 5.185017\n\n\nLet’s see how the confident about the location of the mean changes based on data size. First, we will extract the first 10 cases and re-estimate the model:\n\nN &lt;- 10\ndN &lt;- d2[1:N, ]\nmN &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a+b*(weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=dN\n)\n\nPlot 20 of these lines to see what the uncertainty looks like:\n\npost &lt;- extract.samples(mN, n=20)\n\n# plot the 10 sampled cases\nplot(dN$weight, dN$height, \n     xlim=range(d2$weight), ylim=range(d2$height),\n     col=rangi2, xlab=\"weight\", ylab=\"height\")\n\nmtext(concat(\"N = \", N))\n\nfor(i in 1:20) curve(post$a[i] + post$b[i] * (x-mean(dN$weight)),                col=col.alpha(\"black\", 0.3), add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nLet’s find the quadratic posterior distribution of the mean height \\(\\mu\\) when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean:\n\npost &lt;- extract.samples(m4.3)\nmu_at_50 &lt;- post$a + post$b * (50-xbar)\ndens(mu_at_50, col=rangi2, lwd=2, xlab=\"mu|weight=50\")\n\n\n\n\n\n\n\n\nCompatibility interval of \\(\\mu\\) at 50 kg is:\n\nPI(mu_at_50, prob = 0.89)\n\n      5%      94% \n158.5839 159.6832 \n\n\nTo do that for all weight values:\n\nmu &lt;- link(m4.3)\nstr(mu)\n\n num [1:1000, 1:352] 157 157 157 158 157 ...\n\n\nThe resulted matrix contains 352 columns, each corresponds to one row in the d2 data. It contains 1000 rows, each represents a sample. Therefore, the matrix contains a distribution of \\(\\mu\\) for each individual in the original data d2.\nLet’s plot the Gaussian distribution for each mean value:\n\nplot(height ~ weight, d2, type=\"n\")\n\nfor(i in 1:100)  points(d2$weight, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nThe pile of points represents the rows.\nThe plot is kind of missy, let’s do that for a small group of weight values\n\nweight.seq &lt;- seq(from=25, to=70, by=1)\n\nmu &lt;- link(m4.3, data=data.frame(weight=weight.seq))\n\nplot(height ~ weight, d2, type=\"n\")\n\nfor(i in 1:100)  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nNow, let’s summarize the distribution of mu\n\n# compute the mean of each column (dimension 2) of the matrix mu\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n# plot the line and the interval\nplot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\n\n\n\n\n\n\n\n\n\n\n\nThis approach can be used to generate posterior predictions for any component of any model\n\npost &lt;- extract.samples(m4.3)\nmu.link &lt;- function(weight) post$a + post$b*(weight-xbar)\nweight.seq &lt;- seq(25,70,1)\nmu &lt;- sapply(weight.seq, mu.link)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n\n\n\n\nUse link to generate distributions posterior values for \\(\\mu\\)\nUse mean or PI to find averages and bounds of \\(\\mu\\) for each value of the predictor variable\nPlot the lines and intervals using lines and shades or the distribution of the prediction given the value of predictor(s)\n\n\n\n\n\nWhat we’ve done so far is just use samples from the posterior to visualize the uncertainty in \\(\\mu_i\\). Now, we want to compute the predictions of heights that’s distributed according to: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\nLet’s simulate heights:\n\n# simulate 1e3 data by default\nsim.height &lt;- sim(m4.3, data=list(weight=weight.seq))\nstr(sim.height)\n\n num [1:1000, 1:46] 146 139 136 135 138 ...\n\n\nThe resulted matrix contains 1000 simulated heights (rows) for 46 weight values (columns). Let’s summarize it:\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\nheight.PI\n\n        [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n5%  128.1853 129.4536 130.1615 131.0184 131.5920 132.3106 133.6148 135.5421\n94% 144.4269 145.3213 146.0324 147.5608 148.3459 149.0710 150.5475 151.3630\n        [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n5%  135.8942 136.3520 137.6058 138.6176 139.6518 140.1450 140.8575 141.6930\n94% 151.7098 152.9104 153.9036 154.4739 155.3530 156.1087 157.2042 158.1098\n       [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n5%  143.4392 143.9476 144.7856 145.0695 146.4285 147.0674 148.4928 149.2981\n94% 158.6873 159.7029 160.9985 161.8457 162.9799 163.1741 164.9025 165.5837\n       [,25]    [,26]    [,27]    [,28]    [,29]    [,30]    [,31]    [,32]\n5%  150.1580 151.7772 151.4979 152.9998 154.1103 154.4654 155.6601 156.6560\n94% 165.9044 166.7547 167.7817 168.9641 169.7019 170.2732 171.7155 171.8713\n       [,33]    [,34]    [,35]    [,36]    [,37]    [,38]    [,39]    [,40]\n5%  157.6031 158.3161 159.5117 160.4609 160.9952 161.3552 162.1338 163.4090\n94% 173.9521 174.8402 175.4895 176.2916 177.2782 178.1800 179.4627 180.3177\n       [,41]    [,42]    [,43]    [,44]    [,45]    [,46]\n5%  164.5611 165.1676 165.5014 167.6589 168.1717 168.8493\n94% 180.4383 181.5567 182.5953 183.3265 184.1630 185.3150\n\n\nNow, height.PI contains the 89% (we can use any interval) posterior prediction interval of observable heights across the values of weights in weight.seq (i.e. the boundaries of the simulated heights the model expects)\nLet’s plot everything: 1. the average line (MAP line) 2. shaded region of 89% plausible \\(\\mu\\) 3. boundaries of the simulated heights the model expects\n\n# plot data points\nplot(height ~ weight, d2, col=col.alpha(rangi2, 0.5))\n\n# draw MAP line\nlines(weight.seq, mu.mean)\n\n# i used the border because the shade is not appearing for a bug related to R version \nshade(mu.PI, weight.seq,border=TRUE)\nshade(height.PI, weight.seq, border = TRUE)\n\n\n\n\n\n\n\n\nThe narrow boundaries that are close to the line are the intervals of \\(\\mu\\). The wider boundary is the region within which the model expects to find 89% of actual heights in the population at each weight.\nThe rouglness around the prediction interval is due to the simulation variance. We can decrease that by increasing the number of samples we take from the posterior.\n\nsim.height &lt;- sim(m4.3, data=list(weight=weight.seq), n=1e4)\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\n\n# plot data points\nplot(height ~ weight, d2, col=col.alpha(rangi2, 0.5))\n\n# draw MAP line\nlines(weight.seq, mu.mean)\n\n# i used the border because the shade is not appearing for a bug related to R version \nshade(mu.PI, weight.seq,border=TRUE)\nshade(height.PI, weight.seq, border = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nextract samples from posterior (i.e. parameters values)\nuse the built-in simulation functions like rnorm for Gaussian\n\n\npost &lt;- extract.samples(m4.3)\n\nweight.seq &lt;- 25:70\nsim.height &lt;- sapply(weight.seq, function(weight) \n  rnorm(\n    n=nrow(post),\n    mean=post$a + post$b * (weight - xbar),\n    sd=post$sigma\n  )  \n)\n\nAnd we can summarize it with PI as normal\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\n\n\n\n\nWe can build models to describe the outcome as a curved function of a predictor using the linear regression. Here are the common methods: 1. Polynomial regression 2. B-Splines\n\n\nThe following data is seen to be followed a curved relationship\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nplot(height~weight, d)\n\n\n\n\n\n\n\n\nWe can use the parabolic equation for representing the mean height: \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\)\nThe last parameter \\(\\beta_2\\) measures the curvature of the relationship\nBecause the polynomial equations involve computing the square or curve of large number, we need to standarize the predictor values in order to avoid the errors in computing estimates. To standarize weight values we do the following:\n\\[\nx_{std.} = \\frac{x - \\mu_x}{\\sigma_x}\n\\]\nThis unit is called z-score. However, we will use \\(x\\) instead of \\(x_{std.}\\) in the following sections.\nThis is the definition of our model:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_1 \\sim Log-Normal(0, 1)\\)\n\\(\\beta_2 \\sim Normal(0, 1)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\nNote that it is okay to have negative values for \\(\\beta_2\\).\nLet’s code that and fit the model to our data:\n\nweight_s &lt;- (d$weight - mean(d$weight)) / sd(d$weight)\nweight_s2 &lt;- weight_s ^ 2\n\nm4.5 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b1*weight_s + b2*weight_s2,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\nprecis(m4.5)\n\n            mean        sd       5.5%      94.5%\na     146.056799 0.3689795 145.467099 146.646500\nb1     21.733299 0.2888903  21.271596  22.195001\nb2     -7.803207 0.2741846  -8.241407  -7.365007\nsigma   5.774482 0.1764657   5.492455   6.056508\n\n\nLet’s summarize the prediction and plot it:\n\nweight.seq &lt;- seq(from=-2.2, to=2, length.out=30)\npred_dat &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2)\n\n# compute predictions of mu for pred_dat as input\nmu &lt;- link(m4.5, data=pred_dat)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n# simulate height values\nsim.height &lt;- sim(m4.5, data=pred_dat)\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\n\n\n\n\n\n\n\nRemember that we are now working on the full data with both adults and non-adults, and that’s why the relationship is not linear as it was with the adults data.\nLet’s try building cubic regression on weight:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_1 \\sim Log-Normal(0, 1)\\)\n\\(\\beta_2 \\sim Normal(0, 1)\\)\n\\(\\beta_3 \\sim Normal(0, 1)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\n\nweight_s3 &lt;- weight_s ^ 3\n\nm4.6 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    b3 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\nprecis(m4.6)\n\n            mean        sd       5.5%      94.5%\na     146.394536 0.3099867 145.899117 146.889955\nb1     15.219713 0.4762646  14.458550  15.980875\nb2     -6.202616 0.2571579  -6.613604  -5.791628\nb3      3.583387 0.2287731   3.217763   3.949010\nsigma   4.829882 0.1469421   4.595040   5.064724\n\nweight.seq &lt;- seq(from=-2.2, to=2, length.out=30)\npred_dat_m4.6 &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2, weight_s3=weight.seq^3)\nmu &lt;- link(m4.6, data=pred_dat_m4.6)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\nsim.height &lt;- sim(m4.6, pred_dat_m4.6)\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\n\n\n\n\n\n\n\nThe cubic model is more flexible than others and that’s why it fits well. However, we stoll have these issues in our model:\n\nHaving a better fit \\(\\neq\\) Having a better model\nAll the models we built so far have no biological information. We haven’t learnt any causal relationship so far\n\nThe models are good geocentric model = meaning they describe the sample well\nNote that the x-axis contains the standardized weight values. To convet back to natural scale, we need to remove the current axis and build the axis explicitly:\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5), xaxt=\"n\")\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\nat &lt;- c(-2,-1,0,1,2)\n# convert z-scores to weight values\nlabels &lt;- at*sd(d$weight) + mean(d$weight)\naxis(side=1, at=at, labels=round(labels, 1))\n\n\n\n\n\n\n\n\n\n\n\nB-Spline stands for basis spline. It means that we can build wiggly functions from simple less-wiggly bassis components, that are basis functions\nWe will use data of a 1000 years of blossoms days\n\nlibrary(rethinking)\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nprecis(d, hist=FALSE)\n\n                  mean          sd      5.5%      94.5%\nyear       1408.000000 350.8845964 867.77000 1948.23000\ndoy         104.540508   6.4070362  94.43000  115.00000\ntemp          6.141886   0.6636479   5.15000    7.29470\ntemp_upper    7.185151   0.9929206   5.89765    8.90235\ntemp_lower    5.098941   0.8503496   3.78765    6.37000\n\n\nThe B-Spline model\n\nd2 &lt;- d[complete.cases(d),]\nnum_knots = 15\nknot_list &lt;- quantile(d2$year, probs=seq(0, 1, length.out=num_knots))\n\n\nlibrary(splines)\n# create B-spline basis matrix \nB &lt;- bs(d2$year, \n        knots=knot_list[-c(1, num_knots)], # -c(1, num_knots) means exclude the 1st and last element\n        degree=3,\n        intercept=TRUE)\n\n\n# Create an empty plot with specified axes\n{\n  plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab=\"year\", ylab=\"basis\", type=\"n\")\n  \n  # Plot knots\n  for (knot in knot_list) {\n      # Add a vertical line for each knot\n      abline(v = knot, col = \"red\", lty = 2, lwd = 2)\n  }\n  \n  # Plot each column in the basis matrix against year\n  for (i in 1:ncol(B)) {\n      # Add lines for each column\n      lines(d2$year, B[, i])\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm4.7 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + B %*% w, # matrix multiplication\n    a ~ dnorm(100, 10),\n    w ~ dnorm(0, 1),\n    sigma ~ dexp(1)\n  ), \n  data=list(D=d2$doy, B=B),\n  start=list(w=rep(0, ncol(B)))\n)\n\nLet’s look at the posterior means:\n\nprecis(m4.7)\n\n17 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n            mean        sd       5.5%      94.5%\na     104.705197 0.3330664 104.172893 105.237502\nsigma   6.075087 0.1541852   5.828669   6.321504\n\n\nLet’s plot the posterior predictions:\n\npost &lt;- extract.samples(m4.7)\n# find the mean of all weights\n\nw &lt;- apply(post$w, 2, mean)\n\nplot(NULL, xlim=range(d2$year), ylim=c(-4,4),\n     xlab=\"year\", ylab=\"basis * weight\")\n\n# plot the basis * weight for each column\nfor (i in 1:ncol(B)) lines(d2$year, w[i]*B[,i])\n\n# plot knots\nfor (knot in knot_list) {\n    abline(v = knot, col = \"red\", lty = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n# 97% posterior interval for mu at each year\nmu &lt;- link(m4.7)\nmu.PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col=col.alpha(rangi2, 0.3), pch=16)\nshade(mu.PI, d2$year, col=col.alpha(\"black\", 0.5))\nabline(h = mean(d2$doy, col=\"black\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the following model: \\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu \\sim Normal(0,10\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\nThe likelihood is \\(L = \\prod_i P(y_i | \\mu_i, \\sigma)\\)\nTwo parameters\nThe Bayes theorem for this model is : \\[\nP(\\mu, \\sigma | y) \\propto \\prod_i P(y_i|\\mu, \\sigma) P(\\mu) P(\\sigma)\n\\]\n\n\n\n\nFor the following model:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu \\sim Normal(0,10\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\nSimulate the observed y values from the prior\n\n\nmu.sample &lt;- rnorm(1e3, 0, 10)\nsigma.sample &lt;- rexp(1e3, 1)\ny.sim &lt;- rnorm(1e3, mu.sample, sigma.sample)\n\ndens(y.sim)\n\n\n\n\n\n\n\n\n\nTranslate the model into a quap formula\n\n\n\n\n\n\n\n\nlibrary(rethinking)\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.5.1     v purrr   1.0.2\nv tibble  3.1.6     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\nx purrr::map()    masks rethinking::map()\nx dplyr::select() masks MASS::select()\n\ndata(\"Howell1\")\nd &lt;- Howell1\n\nxbar &lt;- mean(d$weight)\n\nmodel &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu  &lt;-  a + b * (weight - xbar),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\n\nweights &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)\n\n# `extract.samples`: sample parameter values from the posterior\n# `link`: estimate the mean height for each weight\n# `sim`: estimate observations, i.e. sample estimated values\n\nheights &lt;- sim(model, data=data.frame(weight=weights))\nheights.mean &lt;- apply(heights, 2, mean)\nheights.PI &lt;- apply(heights, 2, PI)\nresult &lt;- tibble(weight=weights,\n                 expected_height=heights.mean,\n                 low=heights.PI[1,],\n                 hi=heights.PI[2,],)\n\nresult\n\n# A tibble: 5 x 4\n  weight expected_height   low    hi\n   &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   47.0            158.  143.  173.\n2   43.7            153.  138.  168.\n3   64.8            190.  175.  204.\n4   32.6            133.  118.  148.\n5   54.6            172.  157.  187.\n\n\n\nggplot(result, aes(weights, expected_height, ymin=low, ymax=hi)) +\n  geom_point(size=0.5) +\n  geom_linerange()\n\n\n\n\n\n\n\n\n\nFit the model to data with ages &lt; 18\n\n\nd2 &lt;- d[d$age &lt; 18, ]\n\nxbar2 &lt;- mean(d2$weight)\n\nmodel2 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu  &lt;-  a + b * (weight - xbar2),\n    a ~ dnorm(xbar2, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), \n  data=d2,\n  start=list(a=mean(d2$height), b=3)\n)\n\n(a): For every 10 units of increase in weight, how much taller does the model predict a child gets?\n\nprecis(model2)\n\n            mean         sd       5.5%      94.5%\na     108.235622 0.60868503 107.262826 109.208418\nb       2.716672 0.06831628   2.607489   2.825855\nsigma   8.437275 0.43058790   7.749112   9.125437\n\n\nWhen the weight equals the mean, the expected height (a) is 108.2. For every change in weight of 10 kg, the height is expected to change by 27 cm, with 89% PI of 26 to 28\n(b): plot data, MAP regression line and its 89% PI, and the 89% PI for predicted heights\n\n{\n  # plot d2 data\n  plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5), \n       ylim=c(50, 200), xlim=c(1, 50))\n    \n  weight.seq &lt;- seq(from=4, to=45, by=1)\n  \n  # sample values of mean \n  mu &lt;- link(model2, data=data.frame(weight=weight.seq))\n  # expected mu and 89% PI\n  mu.mean &lt;- apply(mu, 2, mean)\n  mu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n  \n  # plot the line and the PI \n  lines(weight.seq, mu.mean)\n  shade(mu.PI, weight.seq)\n  \n  # sample expected values of height\n  sim.height &lt;- sim(model2, data=list(weight=weight.seq))\n  \n  # plot the line and the PI \n  heights.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n  shade(heights.PI, weight.seq)\n\n}",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html#normal-distribution",
    "href": "stats-rethinking/04-chapter04.html#normal-distribution",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "(Keep in mind the example of field soccer, coin tossing, and stepping right and left on page 72)\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n# given that the steps for each person is represented by a list of 16 random numbers\n# between -1 and 1:\n# run 1000 simulation of stepping left and right, and store the final result/position\npos &lt;- replicate(1000, sum(runif(16,-1,1)))\n\n# plot the end positions around the half line of soccer field\nplot(pos)\n\n\n\n\n\n\n\n# plot the density of the positions\nplot(density(pos))\n\n\n\n\n\n\n\n\n\n\n\n(See the example on page 74) Note that the interaction between the growth deviations converges to Gaussian dist as long as the effect is small.\n\ngrowth_samll_effect &lt;- replicate(1000, prod(1+runif(12,0,0.1)))\ndens(growth_samll_effect, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\ngrowth_big_effect &lt;- replicate(10000, prod(1+runif(12,0,0.5)))\ndens(growth_big_effect, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nMultiplication interactions of large deviations converges to Gaussian dist when we measure the outcomes on the log scale.\n\nlog.big &lt;- replicate(10000, log(prod(1+runif(12,0,0.5))))\ndens(log.big, norm.comp = TRUE)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html#gaussian-model-of-human-height",
    "href": "stats-rethinking/04-chapter04.html#gaussian-model-of-human-height",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "library(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\n\n# explore data\nstr(d)\n\n'data.frame':   544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n\n\n\n# data summary\nprecis(d, hist=FALSE)\n\n              mean         sd      5.5%     94.5%\nheight 138.2635963 27.6024476 81.108550 165.73500\nweight  35.6106176 14.7191782  9.360721  54.50289\nage     29.3443934 20.7468882  1.000000  66.13500\nmale     0.4724265  0.4996986  0.000000   1.00000\n\n\n\n\n\nBased on domain-specific information, we decide that the range of plausible human heights is \\(178 \\mp 40\\). The std. deviation must be basically positive \\(h_i \\sim Normal(\\mu, \\sigma)\\)\nGiven that the parameters are independent, the prior is:\n\\(Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)\\)\nWhere:\n\\(\\mu \\sim Normal(178,20)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\nLet’s plot the priors: - Mean\n\ncurve(dnorm(x, 178, 20), from=100, to=250)\n\n\n\n\n\n\n\n\n\nStd. deviation\n\n\ncurve(dunif(x, 0, 50), from=0, to=60)\n\n\n\n\n\n\n\n\nLet’s simulate heights based on the priors. This is called the prior predictive simulation:\n\nsample_mu &lt;- rnorm(1e4, 178, 20)\nsample_sigma &lt;- runif(1e4, 0, 50)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\n\n\n\n\nSo far, the model is defined before showing it the data. We can change the prior \\(\\mu\\) std. deviation to see how the model is sensitive to the prior choices that aren’t relying on scintific knowledge as we did.\n\nsample_mu &lt;- rnorm(1e4, 178, 100)\nsample_sigma &lt;- runif(1e4, 0, 50)\n\nprior_h &lt;- rnorm(1e4, sample_mu, sample_sigma)\ndens(prior_h)\n\n\n\n\n\n\n\n\nNote how the result doesn’t make sense with negative and very large heights.\n\n\n\nSince we have 2 parameters, grid approx. method is not practical. However, we will try using it computing the log-likelihood:\n\n# we will compute the approximation using the height data of persons over 18 y.o.\nd2 &lt;- d[d$age &gt;= 18,]\n\n\nmu.list &lt;- seq(from=150, to=160, length.out=100)\nsigma.list &lt;- seq(from=7, to=9, length.out=100)\n\n# Create a Data Frame from All Combinations of Factor Variables\npost &lt;- expand.grid(mu=mu.list, sigma=sigma.list)\n\n# compute the log-likelihood\npost$LL &lt;- sapply(\n  1:nrow(post),\n  function(i) sum (\n    dnorm(d2$height, post$mu[i], post$sigma[i], log=TRUE)\n  )\n)\n\npost$prod &lt;- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)\n\n\npost$prob &lt;- exp(post$prod - max(post$prod))\ncontour_xyz(post$mu, post$sigma, post$prob)\n\n\n\n\n\n\n\n\n\nimage_xyz(post$mu, post$sigma, post$prob)\n\n\n\n\n\n\n\n\n\n\n\n\n# generate random indexes of rows\nsample.rows &lt;- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob)\nsample.mu &lt;- post$mu[sample.rows]\nsample.sigma &lt;- post$sigma[sample.rows]\n\n# this shows the most plausible combinations of mu and sigma\nplot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nLet’s check the shape of marginal posterior densities:\n\ndens(sample.mu)\n\n\n\n\n\n\n\ndens(sample.sigma)\n\n\n\n\n\n\n\n\nNote that the density for sigma has a longer right tail\n\nd3 &lt;- sample(d2$height, size = 20)\n\nmu.list &lt;- seq( from=150, to=170 , length.out=200 ) \nsigma.list &lt;- seq( from=4 , to=20 , length.out=200 ) \npost2 &lt;- expand.grid( mu=mu.list , sigma=sigma.list ) \npost2$LL &lt;- sapply( 1:nrow(post2) , function(i) sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) ) \npost2$prod &lt;- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) \npost2$prob &lt;- exp( post2$prod - max(post2$prod) ) \n\nsample2.rows &lt;- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) \nsample2.mu &lt;- post2$mu[ sample2.rows ] \nsample2.sigma &lt;- post2$sigma[ sample2.rows ] \nplot( sample2.mu , sample2.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab=\"mu\" , ylab=\"sigma\" , pch=16 )\n\n\n\n\n\n\n\n\n\ndens(sample2.sigma, norm.comp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Approximation is good to make inferences about the shape of posterior, particularly its peak that lie at the maximum a posteriori (MAP)\nLet’s first load the data:\n\n# load data\nlibrary(rethinking) \ndata(Howell1) \nd &lt;- Howell1 \nd2 &lt;- d[ d$age &gt;= 18 , ]\n\nNow, we will define our model with code:\n\\(h_i \\sim Normal(\\mu, \\sigma)\\)\n\\(\\mu \\sim Normal(178,20)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\n\nflist &lt;- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\nNote: alist stores formulas without executing the expression in the code unlike list\nNow, we fit the mode to the data in `d2`:\n\nm4.1 &lt;- quap(flist, data=d2)\n\nLet’s take a glance at the model (i.e posterior dist):\n\nprecis(m4.1)\n\n            mean        sd       5.5%      94.5%\nmu    154.607026 0.4119947 153.948579 155.265473\nsigma   7.731333 0.2913861   7.265642   8.197025\n\n\n\n\n\n\npost &lt;- extract.samples(m4.1, n=1e4)\nhead(post)\n\n        mu    sigma\n1 154.8719 8.369672\n2 155.0301 8.009201\n3 154.9311 7.360071\n4 154.3732 7.744091\n5 154.7560 7.226322\n6 155.2402 7.903876\n\n\n\nprecis(post, hist=FALSE)\n\n           mean        sd       5.5%      94.5%\nmu    154.60282 0.4124339 153.945159 155.261959\nsigma   7.72796 0.2908551   7.255576   8.187313\n\n\nComparing these values to the output from precis(m4.1), we found it very close.\n\nplot(post)\n\n\n\n\n\n\n\n\n\n\nThe function extract.samples runs the following simulation that samples random vectors of multivariate Gaussian values. This simulation requires computing the variance-covariance matrix\n\nlibrary(MASS)\npost &lt;- mvrnorm(n=1e4, mu=coef(m4.1), Sigma=vcov(m4.1))\nplot(post)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is an essential compnent in the quap algorithm.\nIt tells us how each parameter relates to every other parameter in the posterior distribution.\nIt can be factored into 2 elements:\n\nVector of variances for the parameters diag(vcov(model))\nCorrelation matrix that tells how changes in one parameter lead to correlated changes in the others cov2cor(vcov(model))",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html#linear-prediction",
    "href": "stats-rethinking/04-chapter04.html#linear-prediction",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "Using the association between predictor variables and outcome variable, we want to predict the later. This is how linear regression works.\n\n\n\nWe tell the model (golem) the following: “Assume that the predictor variable has a constant and additive relationship to the mean of the outcome. Consider all the lines (formed by the combinations of parameter values) that relate one variable (or more) to the other. Rank all of these lines by plausibility, given these data.”\nThe resulted model is a posterior distribution\n\nIn the following example, we want to predict the height using the weight as a predictor variable. This code plot the data to use in model fitting:\n\nlibrary(rethinking)\ndata(Howell1)\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18,]\n\nplot(d2$height ~ d2$weight)\n\n\n\n\n\n\n\n\nWe want to use the Gaussian model of height we built in the previous chapters but making the mean of height \\(\\mu_i\\) is a function of weights where weight values are denoted by \\(x\\). Here is the model:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta(x_i - \\bar{x})\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta \\sim Normal(0, 10)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\nNotations:\n\n\\(\\bar{x}\\) is the mean of weights\n\\(x_i\\) weight at row \\(i\\)\n\\(\\mu_i\\) the mean of heights are row \\(i\\)\n\\(h_i\\) the height at row \\(i\\)\n\\(\\alpha, \\beta\\) are parameters to learn\n\nNote all relationships are Stochastic except the relationship between the height mean and weight.\nThe parameters are made up as devices that will help us to manipulate \\(\\mu\\). Here is what each parameter does:\n\n\\(\\alpha\\) (intercept): represents the expected height when \\(x_i=\\bar{x}\\)\n\\(\\beta\\) (slope): represents the rate of change in expectation when \\(x_i\\) changes by 1 unit\n\n\n\n\n\nThe unobserved variables are called parameters (\\(\\alpha, \\beta, \\sigma\\)) and their distributions are called priors.\nEach combination of parameter values implies a unique line\nLet’s simulate the prior predictive distribution to see the possible lines\n\n\nset.seed(2971)\nN &lt;- 100 # 100 lines\na &lt;- rnorm(N, 178, 20)\nb &lt;- rnorm(N, 0, 10)\n\n# prepare the canvas for plotting\nplot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=\"weight\", ylab=\"height\")\nabline(h=0, lty=2) # no one is shorter than zero!\nabline(h=272, lty=1, lwd=0.5) # the world's tallest person\n\nxbar &lt;- mean(d2$weight)\n\n# simulate the possible lines\nfor (i in 1:N) curve(a[i] + b[i]*(x-xbar), \n                     from=min(d2$weight),\n                     to=max(d2$weight),\n                     add=TRUE,\n                     col=col.alpha(\"black\", 0.2))\n\n\n\n\n\n\n\n\nAs we can see, not all the lines seem to represent the relationship between weight and height for human. Negative relationship doesn’t make sense in this context.\nWe want to restrict \\(\\beta\\) to positive numbers so we only get positive relationship. Therefore, we can define the prior as Log-Normal instead to enforce positive relationship:\n\\[\n\\beta \\sim Log-Normal(0,1)\n\\]\n\nb &lt;- rlnorm(1e4, 0, 1)\ndens(b, xlim=c(-1,5), adj=0.1)\n\n\n\n\n\n\n\n\nWe can see the distribution is defined only on the positive beta values.\nNow, let’s do the prior predictive simulation again with the new prior:\n\nset.seed(2971)\nN &lt;- 100 # 100 lines\na &lt;- rnorm(N, 178, 20)\nb &lt;- rlnorm(N, 0, 1) # log-normal prior\n\n# prepare the canvas for plotting\nplot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=\"weight\", ylab=\"height\")\nabline(h=0, lty=2) # no one is shorter than zero!\nabline(h=272, lty=1, lwd=0.5) # the world's tallest person\n\nxbar &lt;- mean(d2$weight)\n\n# simulate the possible lines\nfor (i in 1:N) curve(a[i] + b[i]*(x-xbar), \n                     from=min(d2$weight),\n                     to=max(d2$weight),\n                     add=TRUE,\n                     col=col.alpha(\"black\", 0.2))\n\n\n\n\n\n\n\n\nNow, the result is much more sensible!\n\n\n\nThe model is defined now along with the priors. We are now ready to build the posterior approximation using quap\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18,]\n\nxbar &lt;- mean(d2$weight)\n\n# fit the model\n\nm4.3 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b * (weight-xbar),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data=d2\n)\n\nTo interpret the posterior, we can use either tables or plots. Plots gives more information about the posterior. However, let’s see the summary table:\n\nprecis(m4.3)\n\n             mean         sd        5.5%       94.5%\na     154.6013671 0.27030766 154.1693633 155.0333710\nb       0.9032807 0.04192363   0.8362787   0.9702828\nsigma   5.0718809 0.19115478   4.7663786   5.3773831\n\n\nWe also need to see the covariance among the parameters by computing the variance-covariance matrix:\n\nround(vcov(m4.3), 3)\n\n          a     b sigma\na     0.073 0.000 0.000\nb     0.000 0.002 0.000\nsigma 0.000 0.000 0.037\n\n\n\n\n\nplot(height~weight, data=d2, col=rangi2)\npost &lt;- extract.samples(m4.3)\na_map &lt;- mean(post$a)\nb_map &lt;- mean(post$b)\ncurve(a_map + b_map * (x-xbar), add=TRUE)\n\n\n\n\n\n\n\npost[1:5,]\n\n         a         b    sigma\n1 154.5564 0.9000245 4.768499\n2 154.4602 0.8474034 5.136055\n3 154.7858 0.9440438 5.313512\n4 154.4490 0.8806246 4.683555\n5 154.4639 0.8397176 5.185017\n\n\n\n\n\nWe want to know the uncertainty around the mean of posterior in order to determine the confidence in the relationship between predictor and outcome, since the posterior we plot in the previous step is the MAP, which is the mean of many lines formed by the posterior.\nHere is a sample of possible lines:\n\npost[1:5,]\n\n         a         b    sigma\n1 154.5564 0.9000245 4.768499\n2 154.4602 0.8474034 5.136055\n3 154.7858 0.9440438 5.313512\n4 154.4490 0.8806246 4.683555\n5 154.4639 0.8397176 5.185017\n\n\nLet’s see how the confident about the location of the mean changes based on data size. First, we will extract the first 10 cases and re-estimate the model:\n\nN &lt;- 10\ndN &lt;- d2[1:N, ]\nmN &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a+b*(weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=dN\n)\n\nPlot 20 of these lines to see what the uncertainty looks like:\n\npost &lt;- extract.samples(mN, n=20)\n\n# plot the 10 sampled cases\nplot(dN$weight, dN$height, \n     xlim=range(d2$weight), ylim=range(d2$height),\n     col=rangi2, xlab=\"weight\", ylab=\"height\")\n\nmtext(concat(\"N = \", N))\n\nfor(i in 1:20) curve(post$a[i] + post$b[i] * (x-mean(dN$weight)),                col=col.alpha(\"black\", 0.3), add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\nLet’s find the quadratic posterior distribution of the mean height \\(\\mu\\) when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean:\n\npost &lt;- extract.samples(m4.3)\nmu_at_50 &lt;- post$a + post$b * (50-xbar)\ndens(mu_at_50, col=rangi2, lwd=2, xlab=\"mu|weight=50\")\n\n\n\n\n\n\n\n\nCompatibility interval of \\(\\mu\\) at 50 kg is:\n\nPI(mu_at_50, prob = 0.89)\n\n      5%      94% \n158.5839 159.6832 \n\n\nTo do that for all weight values:\n\nmu &lt;- link(m4.3)\nstr(mu)\n\n num [1:1000, 1:352] 157 157 157 158 157 ...\n\n\nThe resulted matrix contains 352 columns, each corresponds to one row in the d2 data. It contains 1000 rows, each represents a sample. Therefore, the matrix contains a distribution of \\(\\mu\\) for each individual in the original data d2.\nLet’s plot the Gaussian distribution for each mean value:\n\nplot(height ~ weight, d2, type=\"n\")\n\nfor(i in 1:100)  points(d2$weight, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nThe pile of points represents the rows.\nThe plot is kind of missy, let’s do that for a small group of weight values\n\nweight.seq &lt;- seq(from=25, to=70, by=1)\n\nmu &lt;- link(m4.3, data=data.frame(weight=weight.seq))\n\nplot(height ~ weight, d2, type=\"n\")\n\nfor(i in 1:100)  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))\n\n\n\n\n\n\n\n\nNow, let’s summarize the distribution of mu\n\n# compute the mean of each column (dimension 2) of the matrix mu\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n# plot the line and the interval\nplot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\n\n\n\n\n\n\n\n\n\n\n\nThis approach can be used to generate posterior predictions for any component of any model\n\npost &lt;- extract.samples(m4.3)\nmu.link &lt;- function(weight) post$a + post$b*(weight-xbar)\nweight.seq &lt;- seq(25,70,1)\nmu &lt;- sapply(weight.seq, mu.link)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n\n\n\n\nUse link to generate distributions posterior values for \\(\\mu\\)\nUse mean or PI to find averages and bounds of \\(\\mu\\) for each value of the predictor variable\nPlot the lines and intervals using lines and shades or the distribution of the prediction given the value of predictor(s)\n\n\n\n\n\nWhat we’ve done so far is just use samples from the posterior to visualize the uncertainty in \\(\\mu_i\\). Now, we want to compute the predictions of heights that’s distributed according to: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\nLet’s simulate heights:\n\n# simulate 1e3 data by default\nsim.height &lt;- sim(m4.3, data=list(weight=weight.seq))\nstr(sim.height)\n\n num [1:1000, 1:46] 146 139 136 135 138 ...\n\n\nThe resulted matrix contains 1000 simulated heights (rows) for 46 weight values (columns). Let’s summarize it:\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\nheight.PI\n\n        [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n5%  128.1853 129.4536 130.1615 131.0184 131.5920 132.3106 133.6148 135.5421\n94% 144.4269 145.3213 146.0324 147.5608 148.3459 149.0710 150.5475 151.3630\n        [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n5%  135.8942 136.3520 137.6058 138.6176 139.6518 140.1450 140.8575 141.6930\n94% 151.7098 152.9104 153.9036 154.4739 155.3530 156.1087 157.2042 158.1098\n       [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n5%  143.4392 143.9476 144.7856 145.0695 146.4285 147.0674 148.4928 149.2981\n94% 158.6873 159.7029 160.9985 161.8457 162.9799 163.1741 164.9025 165.5837\n       [,25]    [,26]    [,27]    [,28]    [,29]    [,30]    [,31]    [,32]\n5%  150.1580 151.7772 151.4979 152.9998 154.1103 154.4654 155.6601 156.6560\n94% 165.9044 166.7547 167.7817 168.9641 169.7019 170.2732 171.7155 171.8713\n       [,33]    [,34]    [,35]    [,36]    [,37]    [,38]    [,39]    [,40]\n5%  157.6031 158.3161 159.5117 160.4609 160.9952 161.3552 162.1338 163.4090\n94% 173.9521 174.8402 175.4895 176.2916 177.2782 178.1800 179.4627 180.3177\n       [,41]    [,42]    [,43]    [,44]    [,45]    [,46]\n5%  164.5611 165.1676 165.5014 167.6589 168.1717 168.8493\n94% 180.4383 181.5567 182.5953 183.3265 184.1630 185.3150\n\n\nNow, height.PI contains the 89% (we can use any interval) posterior prediction interval of observable heights across the values of weights in weight.seq (i.e. the boundaries of the simulated heights the model expects)\nLet’s plot everything: 1. the average line (MAP line) 2. shaded region of 89% plausible \\(\\mu\\) 3. boundaries of the simulated heights the model expects\n\n# plot data points\nplot(height ~ weight, d2, col=col.alpha(rangi2, 0.5))\n\n# draw MAP line\nlines(weight.seq, mu.mean)\n\n# i used the border because the shade is not appearing for a bug related to R version \nshade(mu.PI, weight.seq,border=TRUE)\nshade(height.PI, weight.seq, border = TRUE)\n\n\n\n\n\n\n\n\nThe narrow boundaries that are close to the line are the intervals of \\(\\mu\\). The wider boundary is the region within which the model expects to find 89% of actual heights in the population at each weight.\nThe rouglness around the prediction interval is due to the simulation variance. We can decrease that by increasing the number of samples we take from the posterior.\n\nsim.height &lt;- sim(m4.3, data=list(weight=weight.seq), n=1e4)\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\n\n# plot data points\nplot(height ~ weight, d2, col=col.alpha(rangi2, 0.5))\n\n# draw MAP line\nlines(weight.seq, mu.mean)\n\n# i used the border because the shade is not appearing for a bug related to R version \nshade(mu.PI, weight.seq,border=TRUE)\nshade(height.PI, weight.seq, border = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nextract samples from posterior (i.e. parameters values)\nuse the built-in simulation functions like rnorm for Gaussian\n\n\npost &lt;- extract.samples(m4.3)\n\nweight.seq &lt;- 25:70\nsim.height &lt;- sapply(weight.seq, function(weight) \n  rnorm(\n    n=nrow(post),\n    mean=post$a + post$b * (weight - xbar),\n    sd=post$sigma\n  )  \n)\n\nAnd we can summarize it with PI as normal\n\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html#curves-from-lines",
    "href": "stats-rethinking/04-chapter04.html#curves-from-lines",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "We can build models to describe the outcome as a curved function of a predictor using the linear regression. Here are the common methods: 1. Polynomial regression 2. B-Splines\n\n\nThe following data is seen to be followed a curved relationship\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nplot(height~weight, d)\n\n\n\n\n\n\n\n\nWe can use the parabolic equation for representing the mean height: \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\)\nThe last parameter \\(\\beta_2\\) measures the curvature of the relationship\nBecause the polynomial equations involve computing the square or curve of large number, we need to standarize the predictor values in order to avoid the errors in computing estimates. To standarize weight values we do the following:\n\\[\nx_{std.} = \\frac{x - \\mu_x}{\\sigma_x}\n\\]\nThis unit is called z-score. However, we will use \\(x\\) instead of \\(x_{std.}\\) in the following sections.\nThis is the definition of our model:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_1 \\sim Log-Normal(0, 1)\\)\n\\(\\beta_2 \\sim Normal(0, 1)\\)\n\\(\\sigma \\sim Uniform(0, 50)\\)\nNote that it is okay to have negative values for \\(\\beta_2\\).\nLet’s code that and fit the model to our data:\n\nweight_s &lt;- (d$weight - mean(d$weight)) / sd(d$weight)\nweight_s2 &lt;- weight_s ^ 2\n\nm4.5 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b1*weight_s + b2*weight_s2,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\nprecis(m4.5)\n\n            mean        sd       5.5%      94.5%\na     146.056799 0.3689795 145.467099 146.646500\nb1     21.733299 0.2888903  21.271596  22.195001\nb2     -7.803207 0.2741846  -8.241407  -7.365007\nsigma   5.774482 0.1764657   5.492455   6.056508\n\n\nLet’s summarize the prediction and plot it:\n\nweight.seq &lt;- seq(from=-2.2, to=2, length.out=30)\npred_dat &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2)\n\n# compute predictions of mu for pred_dat as input\nmu &lt;- link(m4.5, data=pred_dat)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\n# simulate height values\nsim.height &lt;- sim(m4.5, data=pred_dat)\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\n\n\n\n\n\n\n\nRemember that we are now working on the full data with both adults and non-adults, and that’s why the relationship is not linear as it was with the adults data.\nLet’s try building cubic regression on weight:\n\\(h_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\)\n\\(\\alpha \\sim Normal(178, 20)\\)\n\\(\\beta_1 \\sim Log-Normal(0, 1)\\)\n\\(\\beta_2 \\sim Normal(0, 1)\\)\n\\(\\beta_3 \\sim Normal(0, 1)\\)\n\\(\\sigma \\sim Uniform(0,50)\\)\n\nweight_s3 &lt;- weight_s ^ 3\n\nm4.6 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    b3 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\nprecis(m4.6)\n\n            mean        sd       5.5%      94.5%\na     146.394536 0.3099867 145.899117 146.889955\nb1     15.219713 0.4762646  14.458550  15.980875\nb2     -6.202616 0.2571579  -6.613604  -5.791628\nb3      3.583387 0.2287731   3.217763   3.949010\nsigma   4.829882 0.1469421   4.595040   5.064724\n\nweight.seq &lt;- seq(from=-2.2, to=2, length.out=30)\npred_dat_m4.6 &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2, weight_s3=weight.seq^3)\nmu &lt;- link(m4.6, data=pred_dat_m4.6)\nmu.mean &lt;- apply(mu, 2, mean)\nmu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n\nsim.height &lt;- sim(m4.6, pred_dat_m4.6)\nheight.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\n\n\n\n\n\n\n\nThe cubic model is more flexible than others and that’s why it fits well. However, we stoll have these issues in our model:\n\nHaving a better fit \\(\\neq\\) Having a better model\nAll the models we built so far have no biological information. We haven’t learnt any causal relationship so far\n\nThe models are good geocentric model = meaning they describe the sample well\nNote that the x-axis contains the standardized weight values. To convet back to natural scale, we need to remove the current axis and build the axis explicitly:\n\nplot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5), xaxt=\"n\")\nlines(weight.seq, mu.mean)\nshade(mu.PI, weight.seq)\nshade(height.PI, weight.seq)\n\nat &lt;- c(-2,-1,0,1,2)\n# convert z-scores to weight values\nlabels &lt;- at*sd(d$weight) + mean(d$weight)\naxis(side=1, at=at, labels=round(labels, 1))\n\n\n\n\n\n\n\n\n\n\n\nB-Spline stands for basis spline. It means that we can build wiggly functions from simple less-wiggly bassis components, that are basis functions\nWe will use data of a 1000 years of blossoms days\n\nlibrary(rethinking)\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nprecis(d, hist=FALSE)\n\n                  mean          sd      5.5%      94.5%\nyear       1408.000000 350.8845964 867.77000 1948.23000\ndoy         104.540508   6.4070362  94.43000  115.00000\ntemp          6.141886   0.6636479   5.15000    7.29470\ntemp_upper    7.185151   0.9929206   5.89765    8.90235\ntemp_lower    5.098941   0.8503496   3.78765    6.37000\n\n\nThe B-Spline model\n\nd2 &lt;- d[complete.cases(d),]\nnum_knots = 15\nknot_list &lt;- quantile(d2$year, probs=seq(0, 1, length.out=num_knots))\n\n\nlibrary(splines)\n# create B-spline basis matrix \nB &lt;- bs(d2$year, \n        knots=knot_list[-c(1, num_knots)], # -c(1, num_knots) means exclude the 1st and last element\n        degree=3,\n        intercept=TRUE)\n\n\n# Create an empty plot with specified axes\n{\n  plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab=\"year\", ylab=\"basis\", type=\"n\")\n  \n  # Plot knots\n  for (knot in knot_list) {\n      # Add a vertical line for each knot\n      abline(v = knot, col = \"red\", lty = 2, lwd = 2)\n  }\n  \n  # Plot each column in the basis matrix against year\n  for (i in 1:ncol(B)) {\n      # Add lines for each column\n      lines(d2$year, B[, i])\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm4.7 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + B %*% w, # matrix multiplication\n    a ~ dnorm(100, 10),\n    w ~ dnorm(0, 1),\n    sigma ~ dexp(1)\n  ), \n  data=list(D=d2$doy, B=B),\n  start=list(w=rep(0, ncol(B)))\n)\n\nLet’s look at the posterior means:\n\nprecis(m4.7)\n\n17 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n            mean        sd       5.5%      94.5%\na     104.705197 0.3330664 104.172893 105.237502\nsigma   6.075087 0.1541852   5.828669   6.321504\n\n\nLet’s plot the posterior predictions:\n\npost &lt;- extract.samples(m4.7)\n# find the mean of all weights\n\nw &lt;- apply(post$w, 2, mean)\n\nplot(NULL, xlim=range(d2$year), ylim=c(-4,4),\n     xlab=\"year\", ylab=\"basis * weight\")\n\n# plot the basis * weight for each column\nfor (i in 1:ncol(B)) lines(d2$year, w[i]*B[,i])\n\n# plot knots\nfor (knot in knot_list) {\n    abline(v = knot, col = \"red\", lty = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n# 97% posterior interval for mu at each year\nmu &lt;- link(m4.7)\nmu.PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col=col.alpha(rangi2, 0.3), pch=16)\nshade(mu.PI, d2$year, col=col.alpha(\"black\", 0.5))\nabline(h = mean(d2$doy, col=\"black\"))",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/04-chapter04.html#practice",
    "href": "stats-rethinking/04-chapter04.html#practice",
    "title": "4. Geocentric Models",
    "section": "",
    "text": "In the following model: \\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu \\sim Normal(0,10\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\nThe likelihood is \\(L = \\prod_i P(y_i | \\mu_i, \\sigma)\\)\nTwo parameters\nThe Bayes theorem for this model is : \\[\nP(\\mu, \\sigma | y) \\propto \\prod_i P(y_i|\\mu, \\sigma) P(\\mu) P(\\sigma)\n\\]\n\n\n\n\nFor the following model:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu \\sim Normal(0,10\\)\n\\(\\sigma \\sim Exponential(1)\\)\n\nSimulate the observed y values from the prior\n\n\nmu.sample &lt;- rnorm(1e3, 0, 10)\nsigma.sample &lt;- rexp(1e3, 1)\ny.sim &lt;- rnorm(1e3, mu.sample, sigma.sample)\n\ndens(y.sim)\n\n\n\n\n\n\n\n\n\nTranslate the model into a quap formula\n\n\n\n\n\n\n\n\nlibrary(rethinking)\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.5.1     v purrr   1.0.2\nv tibble  3.1.6     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\nx purrr::map()    masks rethinking::map()\nx dplyr::select() masks MASS::select()\n\ndata(\"Howell1\")\nd &lt;- Howell1\n\nxbar &lt;- mean(d$weight)\n\nmodel &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu  &lt;-  a + b * (weight - xbar),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), data=d\n)\n\nweights &lt;- c(46.95, 43.72, 64.78, 32.59, 54.63)\n\n# `extract.samples`: sample parameter values from the posterior\n# `link`: estimate the mean height for each weight\n# `sim`: estimate observations, i.e. sample estimated values\n\nheights &lt;- sim(model, data=data.frame(weight=weights))\nheights.mean &lt;- apply(heights, 2, mean)\nheights.PI &lt;- apply(heights, 2, PI)\nresult &lt;- tibble(weight=weights,\n                 expected_height=heights.mean,\n                 low=heights.PI[1,],\n                 hi=heights.PI[2,],)\n\nresult\n\n# A tibble: 5 x 4\n  weight expected_height   low    hi\n   &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   47.0            158.  143.  173.\n2   43.7            153.  138.  168.\n3   64.8            190.  175.  204.\n4   32.6            133.  118.  148.\n5   54.6            172.  157.  187.\n\n\n\nggplot(result, aes(weights, expected_height, ymin=low, ymax=hi)) +\n  geom_point(size=0.5) +\n  geom_linerange()\n\n\n\n\n\n\n\n\n\nFit the model to data with ages &lt; 18\n\n\nd2 &lt;- d[d$age &lt; 18, ]\n\nxbar2 &lt;- mean(d2$weight)\n\nmodel2 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu  &lt;-  a + b * (weight - xbar2),\n    a ~ dnorm(xbar2, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ), \n  data=d2,\n  start=list(a=mean(d2$height), b=3)\n)\n\n(a): For every 10 units of increase in weight, how much taller does the model predict a child gets?\n\nprecis(model2)\n\n            mean         sd       5.5%      94.5%\na     108.235622 0.60868503 107.262826 109.208418\nb       2.716672 0.06831628   2.607489   2.825855\nsigma   8.437275 0.43058790   7.749112   9.125437\n\n\nWhen the weight equals the mean, the expected height (a) is 108.2. For every change in weight of 10 kg, the height is expected to change by 27 cm, with 89% PI of 26 to 28\n(b): plot data, MAP regression line and its 89% PI, and the 89% PI for predicted heights\n\n{\n  # plot d2 data\n  plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5), \n       ylim=c(50, 200), xlim=c(1, 50))\n    \n  weight.seq &lt;- seq(from=4, to=45, by=1)\n  \n  # sample values of mean \n  mu &lt;- link(model2, data=data.frame(weight=weight.seq))\n  # expected mu and 89% PI\n  mu.mean &lt;- apply(mu, 2, mean)\n  mu.PI &lt;- apply(mu, 2, PI, prob=0.89)\n  \n  # plot the line and the PI \n  lines(weight.seq, mu.mean)\n  shade(mu.PI, weight.seq)\n  \n  # sample expected values of height\n  sim.height &lt;- sim(model2, data=list(weight=weight.seq))\n  \n  # plot the line and the PI \n  heights.PI &lt;- apply(sim.height, 2, PI, prob=0.89)\n  shade(heights.PI, weight.seq)\n\n}",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "4. Geocentric Models"
    ]
  },
  {
    "objectID": "stats-rethinking/06-chapter06.html",
    "href": "stats-rethinking/06-chapter06.html",
    "title": "6. The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "This chapter is about the 3 different hazards that may raise when adding variables to a model: 1. Multicollinearity 2. Post-treatment bias 3. Collider bias\nKnowing about these, the chapter will introduce a careful framework that can tell us which variables we must and must not add to a model in order to arrive at valid inferences.\n\n\n\nset.seed(1914)\nN &lt;- 200\np &lt;- 0.1\n\nnw &lt;- rnorm(N)\ntw &lt;- rnorm(N)\n\ns &lt;- nw + tw\n\n# find the 90th percentile of data\nq &lt;- quantile(s, 1-p)\n\n# keep the points that are higher than the 90th percentile\nselected &lt;- ifelse(s &gt;= q, TRUE, FALSE)\n\ncor(tw[selected], nw[selected])\n\n[1] -0.7680083\n\n# notice the negative correlation as in figure 6.1\n\n\n\n\nWhen two predictors are very strongly correlated, i.e. have the same information (conditional on other variables in the model), including both in a model may lead to confusion. The posterior will be able to make good predictions, but won’t be able to make any claims about which leg is more important (i.e. the posterior dist of coefficients would be weird).\nThis is because the multiple linear regression answers the question:\n\nWhat is the value of knowing each predictor, after already knowing all of the other predictors?\n\n\n\n\nN &lt;- 100\nset.seed(909)\nheight &lt;- rnorm(N,10,2)\nleg_prop &lt;- runif(N,0.4,0.5)  # leg as prop of height\n\nleg_left &lt;- leg_prop * height + rnorm(N,0,0.02)  # sim left leg as proportion + error\nleg_right &lt;- leg_prop * height + rnorm(N,0,0.02)  # sim right leg as proportion + error\n\nd &lt;- data.frame(height, leg_left, leg_right)\n\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nm6.1 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + bl*leg_left + br*leg_right,\n    bl ~ dnorm(2,10),\n    br ~ dnorm(2,10),\n    a ~ dnorm(10,100),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nplot(precis(m6.1))\n\n\n\n\n\n\n\n\nNotice how the interval is very wide for both coefficients bl and br and also contains zero.\nLet’s check the joint posterior dist. of both leg lengths\n\npost &lt;- extract.samples(m6.1)\nplot(bl~br, post, col=col.alpha(rangi2,0.1), pch=16)\n\n\n\n\n\n\n\n\nSince both leg variables contain almost exactly the same information, then they cannot be pulled apart because they never separately influence the posterior mean. The posterior dist. in this example produced a good estimate of the sum of bl and br:\n\\[\n\\mu_i = \\alpha+(\\beta_1 + \\beta_2)x_i\n\\]\n\nsum_blbr &lt;- post$bl + post$br\ndens(sum_blbr, col=rangi2, lwd=2, xlab=\"sum of bl and br\")\n\n\n\n\n\n\n\n\nMean of the sum approx. equals 2\n\nlibrary(rethinking)\nm6.2 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + bl*leg_left,\n    bl ~ dnorm(2,10),\n    a ~ dnorm(10,100),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\nplot(precis(m6.2))\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\n\nError in plot.window(xlim = xlim, ylim = ylim, log = log): need finite 'xlim' values\n\n\n\n\n\n\n\n\n\n\n\n\nThis example discusses the causal aspect of multicollinearity.\n\ndata(\"milk\")\nd &lt;- milk\nd$K &lt;- standardize(d$kcal.per.g)\nd$F &lt;- standardizeDSD(d$perc.fat)  # standardized percent of fat\n\nError in standardizeDSD(d$perc.fat): could not find function \"standardizeDSD\"\n\nd$L &lt;- standardize(d$perc.lactose) # standardized percent of lactose\n\nWe want to model the total energy content using the predictors F and K.\nLet’s start simple and build two models for each predictor:\n\nm6.3 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bF*F,\n    bF ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nm6.4 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bL*L,\n    bL ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nprecis(m6.3)\n\n              mean        sd       5.5%     94.5%\nbF    7.655557e-07 0.5000000 -0.7990958 0.7990973\na     5.327199e-08 0.1335867 -0.2134972 0.2134973\nsigma 9.666296e-01 0.1238653  0.7686689 1.1645903\n\n\n\nplot(precis(m6.3))\n\n\n\n\n\n\n\n\n\nprecis(m6.4)\n\n               mean         sd       5.5%      94.5%\nbL    -9.024560e-01 0.07132850 -1.0164527 -0.7884593\na      6.259896e-07 0.06661636 -0.1064652  0.1064664\nsigma  3.804654e-01 0.04958264  0.3012228  0.4597081\n\n\n\nplot(precis(m6.4))\n\n\n\n\n\n\n\n\nIt seems that both models are mirror images of one another, where the posterior mean of bF is positive and for bL is negative and each has a strong association with the outcome.\nLet’s see what happen when we use both predictors in a model:\n\nm6.5 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bL*L + bF*F,\n    bL ~ dnorm(0,0.5),\n    bF ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nprecis(m6.5)\n\n               mean         sd       5.5%      94.5%\nbL    -9.024568e-01 0.07132824 -1.0164531 -0.7884605\nbF    -1.587206e-06 0.50000000 -0.7990982  0.7990950\na      6.291522e-07 0.06661615 -0.1064648  0.1064661\nsigma  3.804641e-01 0.04958221  0.3012222  0.4597061\n\n\nNotice that the std. dev. now is approx. doubled and the means are close to zero (i.e. the association isn’t strong between predictors and outcome). Let’s explore the variables further:\n\npairs(\n  ~kcal.per.g + perc.fat + perc.lactose, data=d, col=rangi2\n)\n\n\n\n\n\n\n\n\nIt is clear that L and F are strongly associated and that’s why the multicollinearity happened since either predictors helps in predicting kcal.per.g but neither helps as much once you already know the other.\nAway from statistics, there is a confounder that influences L and F making them correlated (see p.169).\n\n\n\n\nDo EDA (e.g. scatterplot matrices) before modelling to see the existing correlations between variables and hence deciding which variables to use as predictors, since it is bad to use redundant variables that are highly correlate as predictors.\nDoing EDA (e.g. scatterplot matrices) and finding the pairwise correlation before modelling to see the existing highly correlated predictors and drop them isn’t enough. It is the conditional associations that matter, not correlation. Also, the associations within the data alone are not enough to decide what to do.\n\nIn other words, there is a confounder that makes predictors/variables associated and this is out of the data and relevant to the scientific model.\n\nNon-identifiability: It is a family of problems that occur in fitted models. It refers to the fact that the structure of data and model don’t make it possible to estimate the parameter’s value\n\nMulticollinearity is a member of Non-identifiability\nWhen this happens with Bayesian models, the resulted posterior dist will be very similar to prior, so comparing both would be a good idea to see how much information the model extracted from the data.\nWhen posterior and prior are similar, it doesn’t mean that the calculation is wrong, but that’s an indicator to ask a better questions since the model answers the questions you are asking.\n\n\n\n\n\n\nIncluding the post-treatment variables can actually mask the treatment itself.\nWhen your goal is to make a causal inference about the treatment, you shouldn’t include the post-treatment variable because it represents the post-treatment effect.\nExample: say in an RCT we want to test the effect of treatment (anti fungus) on plant growth knowing that it targets the fungus. The variables would be:\n\nInitial height (measured before the treatment)\nTreatment (influences the presence of fungus)\nFungus existence (post-treatment effect and measured after applying the treatment)\nFinal height (measured after applying the treatment)\n\nSimulate data:\n\nset.seed(71)\n\nN &lt;- 100\n\n# initial height\nh0 &lt;- rnorm(N,10,2)\n\n# treatment assignment\ntreatment &lt;- rep(0:1, eac=N/2)\n\n# fungus\nfungus &lt;- rbinom(N, size=1, prob=0.5 - treatment*0.4)\n\n# final hieght after growth (remember that the fungus impacts the growth)\nh1 &lt;- h0 + rnorm(N, 5 - 3*fungus)\n\nd &lt;- data.frame(h0=h0, h1=h1, treatment=treatment, fungus=fungus)\n\nprecis(d, hist=FALSE)\n\n              mean        sd      5.5%    94.5%\nh0         9.95978 2.1011623  6.570328 13.07874\nh1        14.39920 2.6880870 10.618002 17.93369\ntreatment  0.50000 0.5025189  0.000000  1.00000\nfungus     0.23000 0.4229526  0.000000  1.00000\n\n\nWhen modeling, we want to add a variable representing the proportion of growth which is 0 &lt; p &lt; 1. So, it is good to have it distributed according to log-normal dist.\n\nm6.6 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu, sigma),\n    mu &lt;- p*h0,\n    p ~ dlnorm(0, 0.25),\n    sigma~dexp(1)\n  ), data=d\n)\n\nprecis(m6.6)\n\n          mean         sd     5.5%    94.5%\np     1.426628 0.01759834 1.398503 1.454754\nsigma 1.792106 0.12496794 1.592383 1.991829\n\n\nSo the average growth is 40%.\nNow, let’s include the treatment and fungus variables to model. By doing so, the proportion of growth will be a function of treatment and fungus presence.\n\nm6.7 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu,sigma),\n    mu &lt;- h0*p,\n    p &lt;- a + bt*treatment * bf*fungus,\n    a ~ dlnorm(0,0.25),\n    bt ~ dnorm(0,0.5),\n    bf ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  \n  ),data=d\n)\n\nprecis(m6.7)\n\n            mean         sd       5.5%     94.5%\na      1.4309578 0.01772404  1.4026314 1.4592843\nbt    -0.3413073 0.28431884 -0.7957037 0.1130892\nbf     0.3413271 0.28433160 -0.1130897 0.7957439\nsigma  1.7709866 0.12378325  1.5731570 1.9688161\n\n\n\nplot(precis(m6.7))\n\n\n\n\n\n\n\n\nThe treatment seems not to be associated a lot with growth unlike what we expect.\nLet’s try removing the fungus variable\n\nm6.8 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu,sigma),\n    mu &lt;- h0*p,\n    p &lt;- a + bt*treatment,\n    a ~ dlnorm(0,0.25),\n    bt ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  \n  ),data=d\n)\n\nplot(precis(m6.8))\n\n\n\n\n\n\n\n\nNow, the interval is more tight and doesn’t include zero which means the association between treatment and growth now is stronger comparing with the result of the previous model. This is because including post-treatment variables can mask the treatment itself.\n\n\n\n\n\n\nsim_x &lt;- rnorm(1e3, 0, 1)\nsim_z &lt;- sim_x + rnorm(1e3, 0, 0.01)\nsim_y &lt;- sim_z + rnorm(1e3, 0, 4)\n\ncor(sim_x, sim_z)\n\n[1] 0.9999495\n\n\n\nd &lt;- data.frame(x=sim_x, y=sim_y, z=sim_z)\nm2 &lt;- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu &lt;- a + bz*z,\n    bz ~ dnorm(0,1),\n    a ~ dnorm(0,1),\n    sigma ~ dexp(1),\n    \n    z~dnorm(mu_z, sigma_z),\n    mu_z &lt;- a_z + bx*x,\n    bx ~ dnorm(0,1),\n    a_z ~ dnorm(0,1),\n    sigma_z ~ dexp(1)\n  ),\n  data = d\n)\n\nplot(precis(m2))\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\n\nError in plot.window(xlim = xlim, ylim = ylim, log = log): need finite 'xlim' values\n\n\n\n\n\n\n\n\n\nThe multicollinearity is here as y and x provide redundant information. In the leg example, the DAG is L &lt;- H -&gt; R and here the DAG is X -&gt; Z -&gt; Y. However, the conditional independence is different although the predictors are highly correlated.\n\n\n\n\ndata(\"WaffleDivorce\")\nd &lt;- WaffleDivorce\n\n# load data and standardaize\nd$S = as.integer(d$South)\nd$M = standardize(d$Marriage)\nd$A = standardize(d$MedianAgeMarriage)\nd$W = standardize(d$WaffleHouses)\nd$D = standardize(d$Divorce)\n\nLet’s create the DAG as in p. 187\n\nlibrary(dagitty)\ndag &lt;- dagitty(\"dag{\n    S -&gt; W -&gt; D;\n    S -&gt; M -&gt; D;\n    S -&gt; A -&gt; D;\n    S -&gt; A -&gt; M -&gt; D\n}\")\n\nLet’s check the adjustment sets:\n\nadjustmentSets(dag, exposure = \"W\", outcome = \"D\")\n\n{ A, M }\n{ S }\n\n\nThese act as confounders. Let’s adjust for S by Including it as a covariate in the regression model.\n\nmh1 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bW*W + bS*S,\n    a ~ dnorm(0, 0.2),\n    bW ~ dnorm(0, 0.5),\n    bS ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n\n  ),data = d\n)\n\n\nplot(precis(mh1))\n\n\n\n\n\n\n\n\nWe can see that the causal effect of W on D is very small which isnot surprising.\n\n\n\n\nimpliedConditionalIndependencies(dag)\n\nA _||_ W | S\nD _||_ S | A, M, W\nM _||_ W | S\n\n\n\n\n\n\ndata(\"foxes\")\nd &lt;- foxes\nstr(d)\n\n'data.frame':   116 obs. of  5 variables:\n $ group    : int  1 1 2 2 3 3 4 4 5 5 ...\n $ avgfood  : num  0.37 0.37 0.53 0.53 0.49 0.49 0.45 0.45 0.74 0.74 ...\n $ groupsize: int  2 2 2 2 2 2 2 2 3 3 ...\n $ area     : num  1.09 1.09 2.05 2.05 2.12 2.12 1.29 1.29 3.78 3.78 ...\n $ weight   : num  5.02 2.84 5.33 6.07 5.85 3.25 4.53 4.09 6.13 5.59 ...\n\n\n\nd$av &lt;- standardize(d$avgfood)\nd$g &lt;- standardize(d$groupsize)\nd$a &lt;- standardize(d$area)\nd$w &lt;- standardize(d$weight)\n\nd$W\n\nNULL\n\nmh3 &lt;- quap(\n  alist(\n    # g -&gt; w &lt;- av\n    w ~ dnorm(mu,sigma),\n    mu &lt;- a + bAV*av + bG*g,\n    bAV ~ dnorm(0,1),\n    bG ~ dnorm(0,1),\n    a ~ dnorm(0, 1),\n    sigma ~ dexp(1),\n    \n    # a -&gt; av\n    av ~ dnorm(muAV, sigmaAV),\n    muAV &lt;- aAV + bA*a,\n    bA ~ dnorm(0,1),\n    aAV ~ dnorm(0, 1),\n    sigmaAV ~ dexp(1),\n    \n    # av -&gt; g\n    g ~ dnorm(muG, sigmaG),\n    muG &lt;- aG + bAG*g,\n    bAG ~ dnorm(0,1),\n    aG ~ dnorm(0, 1),\n    sigmaG ~ dexp(1)\n    \n  ), data=d\n)\n\nError in quap(alist(w ~ dnorm(mu, sigma), mu &lt;- a + bAV * av + bG * g, : non-finite finite-difference value [9]\nStart values for parameters may be too far from MAP.\nTry better priors or use explicit start values.\nIf you sampled random start values, just trying again may work.\nStart values used in this attempt:\nbAV = 0.30243871856132\nbG = 0.925991669299026\nsigma = 0.641444138716906\nbA = -0.809961932175989\naAV = -0.667753358831943\nsigmaAV = 0.679275907576084\nbAG = -1.17343224114012\naG = 0.542118866918761\nsigmaG = 0.44711409509182",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "6. The Haunted DAG & The Causal Terror"
    ]
  },
  {
    "objectID": "stats-rethinking/06-chapter06.html#example-simulated-science-distortion",
    "href": "stats-rethinking/06-chapter06.html#example-simulated-science-distortion",
    "title": "6. The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "set.seed(1914)\nN &lt;- 200\np &lt;- 0.1\n\nnw &lt;- rnorm(N)\ntw &lt;- rnorm(N)\n\ns &lt;- nw + tw\n\n# find the 90th percentile of data\nq &lt;- quantile(s, 1-p)\n\n# keep the points that are higher than the 90th percentile\nselected &lt;- ifelse(s &gt;= q, TRUE, FALSE)\n\ncor(tw[selected], nw[selected])\n\n[1] -0.7680083\n\n# notice the negative correlation as in figure 6.1",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "6. The Haunted DAG & The Causal Terror"
    ]
  },
  {
    "objectID": "stats-rethinking/06-chapter06.html#multicollinearity",
    "href": "stats-rethinking/06-chapter06.html#multicollinearity",
    "title": "6. The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "When two predictors are very strongly correlated, i.e. have the same information (conditional on other variables in the model), including both in a model may lead to confusion. The posterior will be able to make good predictions, but won’t be able to make any claims about which leg is more important (i.e. the posterior dist of coefficients would be weird).\nThis is because the multiple linear regression answers the question:\n\nWhat is the value of knowing each predictor, after already knowing all of the other predictors?\n\n\n\n\nN &lt;- 100\nset.seed(909)\nheight &lt;- rnorm(N,10,2)\nleg_prop &lt;- runif(N,0.4,0.5)  # leg as prop of height\n\nleg_left &lt;- leg_prop * height + rnorm(N,0,0.02)  # sim left leg as proportion + error\nleg_right &lt;- leg_prop * height + rnorm(N,0,0.02)  # sim right leg as proportion + error\n\nd &lt;- data.frame(height, leg_left, leg_right)\n\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\nm6.1 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + bl*leg_left + br*leg_right,\n    bl ~ dnorm(2,10),\n    br ~ dnorm(2,10),\n    a ~ dnorm(10,100),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nplot(precis(m6.1))\n\n\n\n\n\n\n\n\nNotice how the interval is very wide for both coefficients bl and br and also contains zero.\nLet’s check the joint posterior dist. of both leg lengths\n\npost &lt;- extract.samples(m6.1)\nplot(bl~br, post, col=col.alpha(rangi2,0.1), pch=16)\n\n\n\n\n\n\n\n\nSince both leg variables contain almost exactly the same information, then they cannot be pulled apart because they never separately influence the posterior mean. The posterior dist. in this example produced a good estimate of the sum of bl and br:\n\\[\n\\mu_i = \\alpha+(\\beta_1 + \\beta_2)x_i\n\\]\n\nsum_blbr &lt;- post$bl + post$br\ndens(sum_blbr, col=rangi2, lwd=2, xlab=\"sum of bl and br\")\n\n\n\n\n\n\n\n\nMean of the sum approx. equals 2\n\nlibrary(rethinking)\nm6.2 &lt;- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu &lt;- a + bl*leg_left,\n    bl ~ dnorm(2,10),\n    a ~ dnorm(10,100),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\nplot(precis(m6.2))\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\n\nError in plot.window(xlim = xlim, ylim = ylim, log = log): need finite 'xlim' values\n\n\n\n\n\n\n\n\n\n\n\n\nThis example discusses the causal aspect of multicollinearity.\n\ndata(\"milk\")\nd &lt;- milk\nd$K &lt;- standardize(d$kcal.per.g)\nd$F &lt;- standardizeDSD(d$perc.fat)  # standardized percent of fat\n\nError in standardizeDSD(d$perc.fat): could not find function \"standardizeDSD\"\n\nd$L &lt;- standardize(d$perc.lactose) # standardized percent of lactose\n\nWe want to model the total energy content using the predictors F and K.\nLet’s start simple and build two models for each predictor:\n\nm6.3 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bF*F,\n    bF ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\nm6.4 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bL*L,\n    bL ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nprecis(m6.3)\n\n              mean        sd       5.5%     94.5%\nbF    7.655557e-07 0.5000000 -0.7990958 0.7990973\na     5.327199e-08 0.1335867 -0.2134972 0.2134973\nsigma 9.666296e-01 0.1238653  0.7686689 1.1645903\n\n\n\nplot(precis(m6.3))\n\n\n\n\n\n\n\n\n\nprecis(m6.4)\n\n               mean         sd       5.5%      94.5%\nbL    -9.024560e-01 0.07132850 -1.0164527 -0.7884593\na      6.259896e-07 0.06661636 -0.1064652  0.1064664\nsigma  3.804654e-01 0.04958264  0.3012228  0.4597081\n\n\n\nplot(precis(m6.4))\n\n\n\n\n\n\n\n\nIt seems that both models are mirror images of one another, where the posterior mean of bF is positive and for bL is negative and each has a strong association with the outcome.\nLet’s see what happen when we use both predictors in a model:\n\nm6.5 &lt;- quap(\n  alist(\n    K ~ dnorm(mu, sigma),\n    mu &lt;- a + bL*L + bF*F,\n    bL ~ dnorm(0,0.5),\n    bF ~ dnorm(0,0.5),\n    a ~ dnorm(0,0.2),\n    sigma ~ dexp(1)\n  ),\n  data=d\n)\n\n\nprecis(m6.5)\n\n               mean         sd       5.5%      94.5%\nbL    -9.024568e-01 0.07132824 -1.0164531 -0.7884605\nbF    -1.587206e-06 0.50000000 -0.7990982  0.7990950\na      6.291522e-07 0.06661615 -0.1064648  0.1064661\nsigma  3.804641e-01 0.04958221  0.3012222  0.4597061\n\n\nNotice that the std. dev. now is approx. doubled and the means are close to zero (i.e. the association isn’t strong between predictors and outcome). Let’s explore the variables further:\n\npairs(\n  ~kcal.per.g + perc.fat + perc.lactose, data=d, col=rangi2\n)\n\n\n\n\n\n\n\n\nIt is clear that L and F are strongly associated and that’s why the multicollinearity happened since either predictors helps in predicting kcal.per.g but neither helps as much once you already know the other.\nAway from statistics, there is a confounder that influences L and F making them correlated (see p.169).\n\n\n\n\nDo EDA (e.g. scatterplot matrices) before modelling to see the existing correlations between variables and hence deciding which variables to use as predictors, since it is bad to use redundant variables that are highly correlate as predictors.\nDoing EDA (e.g. scatterplot matrices) and finding the pairwise correlation before modelling to see the existing highly correlated predictors and drop them isn’t enough. It is the conditional associations that matter, not correlation. Also, the associations within the data alone are not enough to decide what to do.\n\nIn other words, there is a confounder that makes predictors/variables associated and this is out of the data and relevant to the scientific model.\n\nNon-identifiability: It is a family of problems that occur in fitted models. It refers to the fact that the structure of data and model don’t make it possible to estimate the parameter’s value\n\nMulticollinearity is a member of Non-identifiability\nWhen this happens with Bayesian models, the resulted posterior dist will be very similar to prior, so comparing both would be a good idea to see how much information the model extracted from the data.\nWhen posterior and prior are similar, it doesn’t mean that the calculation is wrong, but that’s an indicator to ask a better questions since the model answers the questions you are asking.",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "6. The Haunted DAG & The Causal Terror"
    ]
  },
  {
    "objectID": "stats-rethinking/06-chapter06.html#post-treatment-bias",
    "href": "stats-rethinking/06-chapter06.html#post-treatment-bias",
    "title": "6. The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "Including the post-treatment variables can actually mask the treatment itself.\nWhen your goal is to make a causal inference about the treatment, you shouldn’t include the post-treatment variable because it represents the post-treatment effect.\nExample: say in an RCT we want to test the effect of treatment (anti fungus) on plant growth knowing that it targets the fungus. The variables would be:\n\nInitial height (measured before the treatment)\nTreatment (influences the presence of fungus)\nFungus existence (post-treatment effect and measured after applying the treatment)\nFinal height (measured after applying the treatment)\n\nSimulate data:\n\nset.seed(71)\n\nN &lt;- 100\n\n# initial height\nh0 &lt;- rnorm(N,10,2)\n\n# treatment assignment\ntreatment &lt;- rep(0:1, eac=N/2)\n\n# fungus\nfungus &lt;- rbinom(N, size=1, prob=0.5 - treatment*0.4)\n\n# final hieght after growth (remember that the fungus impacts the growth)\nh1 &lt;- h0 + rnorm(N, 5 - 3*fungus)\n\nd &lt;- data.frame(h0=h0, h1=h1, treatment=treatment, fungus=fungus)\n\nprecis(d, hist=FALSE)\n\n              mean        sd      5.5%    94.5%\nh0         9.95978 2.1011623  6.570328 13.07874\nh1        14.39920 2.6880870 10.618002 17.93369\ntreatment  0.50000 0.5025189  0.000000  1.00000\nfungus     0.23000 0.4229526  0.000000  1.00000\n\n\nWhen modeling, we want to add a variable representing the proportion of growth which is 0 &lt; p &lt; 1. So, it is good to have it distributed according to log-normal dist.\n\nm6.6 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu, sigma),\n    mu &lt;- p*h0,\n    p ~ dlnorm(0, 0.25),\n    sigma~dexp(1)\n  ), data=d\n)\n\nprecis(m6.6)\n\n          mean         sd     5.5%    94.5%\np     1.426628 0.01759834 1.398503 1.454754\nsigma 1.792106 0.12496794 1.592383 1.991829\n\n\nSo the average growth is 40%.\nNow, let’s include the treatment and fungus variables to model. By doing so, the proportion of growth will be a function of treatment and fungus presence.\n\nm6.7 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu,sigma),\n    mu &lt;- h0*p,\n    p &lt;- a + bt*treatment * bf*fungus,\n    a ~ dlnorm(0,0.25),\n    bt ~ dnorm(0,0.5),\n    bf ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  \n  ),data=d\n)\n\nprecis(m6.7)\n\n            mean         sd       5.5%     94.5%\na      1.4309578 0.01772404  1.4026314 1.4592843\nbt    -0.3413073 0.28431884 -0.7957037 0.1130892\nbf     0.3413271 0.28433160 -0.1130897 0.7957439\nsigma  1.7709866 0.12378325  1.5731570 1.9688161\n\n\n\nplot(precis(m6.7))\n\n\n\n\n\n\n\n\nThe treatment seems not to be associated a lot with growth unlike what we expect.\nLet’s try removing the fungus variable\n\nm6.8 &lt;- quap(\n  alist(\n    h1 ~ dnorm(mu,sigma),\n    mu &lt;- h0*p,\n    p &lt;- a + bt*treatment,\n    a ~ dlnorm(0,0.25),\n    bt ~ dnorm(0,0.5),\n    sigma ~ dexp(1)\n  \n  ),data=d\n)\n\nplot(precis(m6.8))\n\n\n\n\n\n\n\n\nNow, the interval is more tight and doesn’t include zero which means the association between treatment and growth now is stronger comparing with the result of the previous model. This is because including post-treatment variables can mask the treatment itself.",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "6. The Haunted DAG & The Causal Terror"
    ]
  },
  {
    "objectID": "stats-rethinking/06-chapter06.html#exercises",
    "href": "stats-rethinking/06-chapter06.html#exercises",
    "title": "6. The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "sim_x &lt;- rnorm(1e3, 0, 1)\nsim_z &lt;- sim_x + rnorm(1e3, 0, 0.01)\nsim_y &lt;- sim_z + rnorm(1e3, 0, 4)\n\ncor(sim_x, sim_z)\n\n[1] 0.9999495\n\n\n\nd &lt;- data.frame(x=sim_x, y=sim_y, z=sim_z)\nm2 &lt;- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu &lt;- a + bz*z,\n    bz ~ dnorm(0,1),\n    a ~ dnorm(0,1),\n    sigma ~ dexp(1),\n    \n    z~dnorm(mu_z, sigma_z),\n    mu_z &lt;- a_z + bx*x,\n    bx ~ dnorm(0,1),\n    a_z ~ dnorm(0,1),\n    sigma_z ~ dexp(1)\n  ),\n  data = d\n)\n\nplot(precis(m2))\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\nWarning in sqrt(diag(vcov(model))): NaNs produced\n\n\nError in plot.window(xlim = xlim, ylim = ylim, log = log): need finite 'xlim' values\n\n\n\n\n\n\n\n\n\nThe multicollinearity is here as y and x provide redundant information. In the leg example, the DAG is L &lt;- H -&gt; R and here the DAG is X -&gt; Z -&gt; Y. However, the conditional independence is different although the predictors are highly correlated.\n\n\n\n\ndata(\"WaffleDivorce\")\nd &lt;- WaffleDivorce\n\n# load data and standardaize\nd$S = as.integer(d$South)\nd$M = standardize(d$Marriage)\nd$A = standardize(d$MedianAgeMarriage)\nd$W = standardize(d$WaffleHouses)\nd$D = standardize(d$Divorce)\n\nLet’s create the DAG as in p. 187\n\nlibrary(dagitty)\ndag &lt;- dagitty(\"dag{\n    S -&gt; W -&gt; D;\n    S -&gt; M -&gt; D;\n    S -&gt; A -&gt; D;\n    S -&gt; A -&gt; M -&gt; D\n}\")\n\nLet’s check the adjustment sets:\n\nadjustmentSets(dag, exposure = \"W\", outcome = \"D\")\n\n{ A, M }\n{ S }\n\n\nThese act as confounders. Let’s adjust for S by Including it as a covariate in the regression model.\n\nmh1 &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- a + bW*W + bS*S,\n    a ~ dnorm(0, 0.2),\n    bW ~ dnorm(0, 0.5),\n    bS ~ dnorm(0, 0.5),\n    sigma ~ dexp(1)\n\n  ),data = d\n)\n\n\nplot(precis(mh1))\n\n\n\n\n\n\n\n\nWe can see that the causal effect of W on D is very small which isnot surprising.\n\n\n\n\nimpliedConditionalIndependencies(dag)\n\nA _||_ W | S\nD _||_ S | A, M, W\nM _||_ W | S\n\n\n\n\n\n\ndata(\"foxes\")\nd &lt;- foxes\nstr(d)\n\n'data.frame':   116 obs. of  5 variables:\n $ group    : int  1 1 2 2 3 3 4 4 5 5 ...\n $ avgfood  : num  0.37 0.37 0.53 0.53 0.49 0.49 0.45 0.45 0.74 0.74 ...\n $ groupsize: int  2 2 2 2 2 2 2 2 3 3 ...\n $ area     : num  1.09 1.09 2.05 2.05 2.12 2.12 1.29 1.29 3.78 3.78 ...\n $ weight   : num  5.02 2.84 5.33 6.07 5.85 3.25 4.53 4.09 6.13 5.59 ...\n\n\n\nd$av &lt;- standardize(d$avgfood)\nd$g &lt;- standardize(d$groupsize)\nd$a &lt;- standardize(d$area)\nd$w &lt;- standardize(d$weight)\n\nd$W\n\nNULL\n\nmh3 &lt;- quap(\n  alist(\n    # g -&gt; w &lt;- av\n    w ~ dnorm(mu,sigma),\n    mu &lt;- a + bAV*av + bG*g,\n    bAV ~ dnorm(0,1),\n    bG ~ dnorm(0,1),\n    a ~ dnorm(0, 1),\n    sigma ~ dexp(1),\n    \n    # a -&gt; av\n    av ~ dnorm(muAV, sigmaAV),\n    muAV &lt;- aAV + bA*a,\n    bA ~ dnorm(0,1),\n    aAV ~ dnorm(0, 1),\n    sigmaAV ~ dexp(1),\n    \n    # av -&gt; g\n    g ~ dnorm(muG, sigmaG),\n    muG &lt;- aG + bAG*g,\n    bAG ~ dnorm(0,1),\n    aG ~ dnorm(0, 1),\n    sigmaG ~ dexp(1)\n    \n  ), data=d\n)\n\nError in quap(alist(w ~ dnorm(mu, sigma), mu &lt;- a + bAV * av + bG * g, : non-finite finite-difference value [9]\nStart values for parameters may be too far from MAP.\nTry better priors or use explicit start values.\nIf you sampled random start values, just trying again may work.\nStart values used in this attempt:\nbAV = 0.30243871856132\nbG = 0.925991669299026\nsigma = 0.641444138716906\nbA = -0.809961932175989\naAV = -0.667753358831943\nsigmaAV = 0.679275907576084\nbAG = -1.17343224114012\naG = 0.542118866918761\nsigmaG = 0.44711409509182",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "6. The Haunted DAG & The Causal Terror"
    ]
  },
  {
    "objectID": "stats-rethinking/99-lectures.html",
    "href": "stats-rethinking/99-lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Let’s make a function that represents the relationships in this scientific (causal) model:\n\n\n# simulation function #\nsim_HW &lt;- function(S,b,a) {\n# S=1 female; S=2 male\n  N &lt;- length(S)\n  # arbirtary parameters based on sex\n  H &lt;- ifelse(S==1, 150, 160) + rnorm(N, 0, 5)\n  \n  # a is the intercept; b is the slope\n  W &lt;- a[S] + b[S] * H + rnorm(N, 0, 5)\n  data.frame(S, H, W)\n}\n\n# test the simulation function #\n\nrbern &lt;- function(n, p = 0.5) {\n  rbinom(n, size = 1, prob = p)\n}\n\n# generate sexes for persons\nS &lt;- rbern(100) + 1\n# pass the parameters a and b for each sex\ndat &lt;- sim_HW(S, b=c(0.5,0.6), a=c(0,0))\nhead(dat)\n\n  S        H         W\n1 1 149.8310  62.27643\n2 1 146.0825  76.61531\n3 2 159.5063  98.03304\n4 2 153.9408  90.19251\n5 1 142.7231  76.50352\n6 2 162.7157 101.02463\n\n\nFinding the causal effect involves conditioning on the confounder or mediator to block the association. This implies computing the difference between posterior prediction which is formally called computing the contrast.\n\n\n\nWhat’s the total causal effect of sex (through the two passes)? It is the difference made intervening. Let’s find that by testing\n\n# female sample\nS &lt;- rep(1,100)\nsimF &lt;- sim_HW(S,b=c(0.5,0.6),a=c(0,0))\n\n# female sample\nS &lt;- rep(2,100)\nsimM &lt;- sim_HW(S,b=c(0.5,0.6),a=c(0,0))\n\n# effect of sex (male-female)\nmean(simM$W - simF$W)\n\n[1] 20.58828\n\n\nNow, we want to define the statistical model (i.e. generative model) of weight:\n\\[\nW_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[ \\mu_i = \\alpha_{S[i]} \\]\n\\[ \\alpha_j \\sim Normal(60,10) \\]\n\\[ \\sigma \\sim Uniform(0,10) \\]\nLet’s run the estimating model and synthetic sample\n\n# observe sample (100 individuals)\nS &lt;- rbern(100)+1\ndat &lt;- sim_HW(S, b=c(.5,.6), a=c(0,0))\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    rbern\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n# estimate posterior\nm_SW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S],\n    a[S] ~ dnorm(60,10),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nprecis(m_SW, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  74.145439 0.6641629 73.083979 75.206900\na[2]  94.912007 0.7492726 93.714525 96.109490\nsigma  4.980696 0.3524650  4.417389  5.544004\n\n\nNote that a is the average weight in the observed sample, stratified by sex.\n\n\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18, ]\n\ndat &lt;- list(\n  W = d$weight,\n  S = d$male + 1\n)\n\nm_SW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S],\n    a[S] ~ dnorm(60,10),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nFind the mean weight for each sex (mean weight, NOT predicted weight!)\n\n# sample from posterior (i.e. sample parameter values)\npost &lt;- extract.samples(m_SW)\n\n# plot the posterior mean weight for each sex\n{\n  # female in red\n  dens(post$a[, 1], xlim=c(39,50), lwd=3, col=2, xlab=\"posterior mean weight (kg)\")\n  dens(post$a[, 2], add=TRUE, col=4)\n}\n\n\n\n\n\n\n\n\n\nW1 &lt;- rnorm(1e3, post$a[,1], post$sigma)\nW2 &lt;- rnorm(1e3, post$a[,2], post$sigma)\n\n\n# plot the posterior predicted weight for each sex\n{\n  # female in red\n  dens(W1, xlim=c(20,70), lwd=3, col=2, xlab=\"posterior mean weight (kg)\")\n  dens(W2, add=TRUE, col=4)\n}\n\n\n\n\n\n\n\n\n\n\n\nTo find the difference between posteriors of each category, we need to compute the contrast.\nNote: overlap of distributions doesn’t indicate that they are the same or different!\n\n\n\nmu_contrast &lt;- post$a[,2] - post$a[,1]\n\ndens(mu_contrast, xlim=c(3,10), lwd=3, col=1, xlab=\"posterior mean weight contrast (kg)\")\n\n\n\n\n\n\n\n\n\n\n\nWe want to find the contrast in the distributions of individual people not averages\n\n# simulate\nW1 &lt;- rnorm(1e3, post$a[,1], post$sigma)\nW2 &lt;- rnorm(1e3, post$a[,2], post$sigma)\n\n# contrast\nW_contrast &lt;- W2 - W1\ndens(W_contrast, xlim=c(-25,35), lwd=3, col=1, xlab=\"posterior weight contrast (kg\")\n\n\n\n\n\n\n\n\nSince we subtracted the posterior of women weights from the posterior of men weights, we can get the proportion at which the men have weight more than women by doing the following:\n\nsum(W_contrast &gt; 0) / 1e3\n\n[1] 0.803\n\n\nSo 79% of the time, men are heavier than women in this population\n\nsum(W_contrast &lt; 0) / 1e3\n\n[1] 0.197\n\n\nSo 20% of the time, women are heavier than women in this population\n\n\n\n\n\n\nWe need to block association through H = This means stratify by H.\nThe model is defined by:\n\\[\nW_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H})\n\\]\nNote: it is a common practice to subtract predictor (i.e. H) from its average in order to make the intercept reflecting the outcome value when the predictor equals the average value. This is called “centering”.\nThe parameter now is a linear model. Let’s define the whole model with code:\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18, ]\n\ndat &lt;- list(\n  W = d$weight,\n  H = d$height,\n  Hbar = mean(d$height),\n  S = d$male+1\n)\n\nm_SHW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60,10),\n    b[S] ~ dunif(0,1),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nError in quap(alist(W ~ dnorm(mu, sigma), mu &lt;- a[S] + b[S] * (H - Hbar), : non-finite finite-difference value [3]\nStart values for parameters may be too far from MAP.\nTry better priors or use explicit start values.\nIf you sampled random start values, just trying again may work.\nStart values used in this attempt:\na = c(67.3159817200256, 64.0591387473614)\nb = c(0.0486216379795223, 0.888933669775724)\nsigma = 3.84191120741889",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "Lectures"
    ]
  },
  {
    "objectID": "stats-rethinking/99-lectures.html#lecture-4-categories-curves",
    "href": "stats-rethinking/99-lectures.html#lecture-4-categories-curves",
    "title": "Lectures",
    "section": "",
    "text": "Let’s make a function that represents the relationships in this scientific (causal) model:\n\n\n# simulation function #\nsim_HW &lt;- function(S,b,a) {\n# S=1 female; S=2 male\n  N &lt;- length(S)\n  # arbirtary parameters based on sex\n  H &lt;- ifelse(S==1, 150, 160) + rnorm(N, 0, 5)\n  \n  # a is the intercept; b is the slope\n  W &lt;- a[S] + b[S] * H + rnorm(N, 0, 5)\n  data.frame(S, H, W)\n}\n\n# test the simulation function #\n\nrbern &lt;- function(n, p = 0.5) {\n  rbinom(n, size = 1, prob = p)\n}\n\n# generate sexes for persons\nS &lt;- rbern(100) + 1\n# pass the parameters a and b for each sex\ndat &lt;- sim_HW(S, b=c(0.5,0.6), a=c(0,0))\nhead(dat)\n\n  S        H         W\n1 1 149.8310  62.27643\n2 1 146.0825  76.61531\n3 2 159.5063  98.03304\n4 2 153.9408  90.19251\n5 1 142.7231  76.50352\n6 2 162.7157 101.02463\n\n\nFinding the causal effect involves conditioning on the confounder or mediator to block the association. This implies computing the difference between posterior prediction which is formally called computing the contrast.\n\n\n\nWhat’s the total causal effect of sex (through the two passes)? It is the difference made intervening. Let’s find that by testing\n\n# female sample\nS &lt;- rep(1,100)\nsimF &lt;- sim_HW(S,b=c(0.5,0.6),a=c(0,0))\n\n# female sample\nS &lt;- rep(2,100)\nsimM &lt;- sim_HW(S,b=c(0.5,0.6),a=c(0,0))\n\n# effect of sex (male-female)\nmean(simM$W - simF$W)\n\n[1] 20.58828\n\n\nNow, we want to define the statistical model (i.e. generative model) of weight:\n\\[\nW_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[ \\mu_i = \\alpha_{S[i]} \\]\n\\[ \\alpha_j \\sim Normal(60,10) \\]\n\\[ \\sigma \\sim Uniform(0,10) \\]\nLet’s run the estimating model and synthetic sample\n\n# observe sample (100 individuals)\nS &lt;- rbern(100)+1\ndat &lt;- sim_HW(S, b=c(.5,.6), a=c(0,0))\n\nlibrary(rethinking)\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/H/Documents/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.40)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    rbern\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n# estimate posterior\nm_SW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S],\n    a[S] ~ dnorm(60,10),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nprecis(m_SW, depth = 2)\n\n           mean        sd      5.5%     94.5%\na[1]  74.145439 0.6641629 73.083979 75.206900\na[2]  94.912007 0.7492726 93.714525 96.109490\nsigma  4.980696 0.3524650  4.417389  5.544004\n\n\nNote that a is the average weight in the observed sample, stratified by sex.\n\n\n\nlibrary(rethinking)\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18, ]\n\ndat &lt;- list(\n  W = d$weight,\n  S = d$male + 1\n)\n\nm_SW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S],\n    a[S] ~ dnorm(60,10),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nFind the mean weight for each sex (mean weight, NOT predicted weight!)\n\n# sample from posterior (i.e. sample parameter values)\npost &lt;- extract.samples(m_SW)\n\n# plot the posterior mean weight for each sex\n{\n  # female in red\n  dens(post$a[, 1], xlim=c(39,50), lwd=3, col=2, xlab=\"posterior mean weight (kg)\")\n  dens(post$a[, 2], add=TRUE, col=4)\n}\n\n\n\n\n\n\n\n\n\nW1 &lt;- rnorm(1e3, post$a[,1], post$sigma)\nW2 &lt;- rnorm(1e3, post$a[,2], post$sigma)\n\n\n# plot the posterior predicted weight for each sex\n{\n  # female in red\n  dens(W1, xlim=c(20,70), lwd=3, col=2, xlab=\"posterior mean weight (kg)\")\n  dens(W2, add=TRUE, col=4)\n}\n\n\n\n\n\n\n\n\n\n\n\nTo find the difference between posteriors of each category, we need to compute the contrast.\nNote: overlap of distributions doesn’t indicate that they are the same or different!\n\n\n\nmu_contrast &lt;- post$a[,2] - post$a[,1]\n\ndens(mu_contrast, xlim=c(3,10), lwd=3, col=1, xlab=\"posterior mean weight contrast (kg)\")\n\n\n\n\n\n\n\n\n\n\n\nWe want to find the contrast in the distributions of individual people not averages\n\n# simulate\nW1 &lt;- rnorm(1e3, post$a[,1], post$sigma)\nW2 &lt;- rnorm(1e3, post$a[,2], post$sigma)\n\n# contrast\nW_contrast &lt;- W2 - W1\ndens(W_contrast, xlim=c(-25,35), lwd=3, col=1, xlab=\"posterior weight contrast (kg\")\n\n\n\n\n\n\n\n\nSince we subtracted the posterior of women weights from the posterior of men weights, we can get the proportion at which the men have weight more than women by doing the following:\n\nsum(W_contrast &gt; 0) / 1e3\n\n[1] 0.803\n\n\nSo 79% of the time, men are heavier than women in this population\n\nsum(W_contrast &lt; 0) / 1e3\n\n[1] 0.197\n\n\nSo 20% of the time, women are heavier than women in this population\n\n\n\n\n\n\nWe need to block association through H = This means stratify by H.\nThe model is defined by:\n\\[\nW_i \\sim Normal(\\mu_i, \\sigma)\n\\]\n\\[\n\\mu_i = \\alpha_{S[i]} + \\beta_{S[i]}(H_i - \\bar{H})\n\\]\nNote: it is a common practice to subtract predictor (i.e. H) from its average in order to make the intercept reflecting the outcome value when the predictor equals the average value. This is called “centering”.\nThe parameter now is a linear model. Let’s define the whole model with code:\n\ndata(\"Howell1\")\nd &lt;- Howell1\nd &lt;- d[d$age &gt;= 18, ]\n\ndat &lt;- list(\n  W = d$weight,\n  H = d$height,\n  Hbar = mean(d$height),\n  S = d$male+1\n)\n\nm_SHW &lt;- quap(\n  alist(\n    W ~ dnorm(mu, sigma),\n    mu &lt;- a[S] + b[S] * (H - Hbar),\n    a[S] ~ dnorm(60,10),\n    b[S] ~ dunif(0,1),\n    sigma ~ dunif(0,10)\n  ),\n  data=dat\n)\n\nError in quap(alist(W ~ dnorm(mu, sigma), mu &lt;- a[S] + b[S] * (H - Hbar), : non-finite finite-difference value [3]\nStart values for parameters may be too far from MAP.\nTry better priors or use explicit start values.\nIf you sampled random start values, just trying again may work.\nStart values used in this attempt:\na = c(67.3159817200256, 64.0591387473614)\nb = c(0.0486216379795223, 0.888933669775724)\nsigma = 3.84191120741889",
    "crumbs": [
      "Statistical Rethinking, 2nd Edition",
      "Lectures"
    ]
  }
]